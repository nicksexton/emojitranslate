{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# A Seq2seq model for generating tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_load_seq2seq_utils as s2s_util\n",
    "import data_load_utils as util\n",
    "from importlib import reload\n",
    "\n",
    "util = reload(util)\n",
    "s2s_util = reload(s2s_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tweets_orig = util.read_tweet_data('data/emojis_homemade.csv')\n",
    "tweets_additional_0 = util.read_tweet_data('data/emojis_additional.csv')\n",
    "\n",
    "tweets=pd.DataFrame.append(tweets_orig, tweets_additional_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = util.filter_tweets_min_count(tweets, min_count=1000)\n",
    "tweets.reset_index()\n",
    "\n",
    "\n",
    "tweets['text'] = util.filter_text_for_handles(tweets['text'])\n",
    "\n",
    "# After the filtering, remember to append a \\n character to each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT [VID] 181023 - Foi adicionada a letra D no ...</td>\n",
       "      <td>Â©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT 181023 Kris Wu Studio update (3/3)Legendary...</td>\n",
       "      <td>ğŸ’«</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT Now you are watching Indian SuperStar with ...</td>\n",
       "      <td>ğŸ˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dats for keeps</td>\n",
       "      <td>ğŸ’›</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Holy shit no I think.</td>\n",
       "      <td>ğŸ˜©</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emoji\n",
       "0  RT [VID] 181023 - Foi adicionada a letra D no ...     Â©\n",
       "1  RT 181023 Kris Wu Studio update (3/3)Legendary...     ğŸ’«\n",
       "2  RT Now you are watching Indian SuperStar with ...     ğŸ˜\n",
       "3                                    dats for keeps      ğŸ’›\n",
       "6                             Holy shit no I think.      ğŸ˜©"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645016, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Define the set of characters that we'll use to encode our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create dicts for character/emoji to index conversion\n",
    "\n",
    "chars_univ, chars_univ_idx = s2s_util.get_universal_chars_list()\n",
    "\n",
    "emojis = sorted(list(set(tweets['emoji'])))\n",
    "emoji_idx = dict((emoji, emojis.index(emoji)) for emoji in emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train_batches: 256.0\n",
      "n_dev_batches: 8.0\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 2**19 # 8192  try 131072 = 2**18 for production\n",
    "DEV_SIZE = 2**14   # 8192  try 8192 = 2**13 for production\n",
    "\n",
    "TWEETS_PER_BATCH = 2048\n",
    "MAX_TWEET_LENGTH = 160\n",
    "n_train_batches = TRAIN_SIZE / TWEETS_PER_BATCH\n",
    "n_dev_batches = DEV_SIZE / TWEETS_PER_BATCH\n",
    "\n",
    "print (\"n_train_batches:\", n_train_batches)\n",
    "print (\"n_dev_batches:\", n_dev_batches)\n",
    "\n",
    "tweets_train = tweets.iloc[0:TRAIN_SIZE] # 8192 = 2**13\n",
    "tweets_dev = tweets.iloc[TRAIN_SIZE:TRAIN_SIZE+DEV_SIZE] # 2048 = 2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524288, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = s2s_util.xy_generator(tweets_train, emoji_indices=emoji_idx)\n",
    "dev_generator = s2s_util.xy_generator(tweets_dev, emoji_indices=emoji_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 161, 94)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([emoj, x], y) = train_generator.__next__()\n",
    "#e = emoj.reshape(64, 1, 111)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we're going to use the algorithm from the Keras example of a seq2seq model.\n",
    "We'll supply the emoji to the encoder LSTM which will encode it into two state vectors,\n",
    "and the decoder LSTM will be trained on the tweets using teacher forcing.\n",
    "\n",
    "\n",
    "\n",
    "# Summary of the algorithm\n",
    "\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "ENCODER_HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, len(emoji_idx)))\n",
    "encoder = LSTM(ENCODER_HIDDEN_SIZE, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, len(chars_univ)))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(ENCODER_HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(chars_univ), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None, 129)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None, 94)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 395264      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 256),  359424      input_4[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 94)     24158       lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 778,846\n",
      "Trainable params: 778,846\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "256/256 [==============================] - 29s 113ms/step - loss: 1.4126 - val_loss: 0.9958\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 28s 109ms/step - loss: 0.8345 - val_loss: 1.1969\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 28s 109ms/step - loss: 0.3606 - val_loss: 1.5655\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 28s 109ms/step - loss: 0.1033 - val_loss: 2.0080\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 28s 109ms/step - loss: 0.0339 - val_loss: 2.3344\n",
      "Epoch 6/100\n",
      "  8/256 [..............................] - ETA: 26s - loss: 0.0126"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e09e1cb240b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_dev_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=n_train_batches,\n",
    "                    epochs=100,\n",
    "                    validation_data=dev_generator,\n",
    "                    validation_steps=n_dev_batches,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save('emoji_s2s.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(ENCODER_HIDDEN_SIZE,))\n",
    "decoder_state_input_c = Input(shape=(ENCODER_HIDDEN_SIZE,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_chars_idx = dict(\n",
    "(i, char) for char, i in chars_univ_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(chars_univ)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, chars_univ_idx['\\n']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_chars_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > MAX_TWEET_LENGTH):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(chars_univ)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_oh(emoji, emoji_idx):\n",
    "    emoji_arr = np.zeros(shape=(1, 1, len(emoji_idx)))\n",
    "    emoji_arr[0, 0, emoji_idx[emoji]] = 1\n",
    "    return emoji_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ’›'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.iloc[3].loc['emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’›\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tsto/re belips \\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make one hot vector for emoji input\n",
    "emoji = tweets_train.iloc[3].loc['emoji']\n",
    "print (emoji)\n",
    "#emoji_arr = np.zeros(shape=(1, 1, len(emoji_idx)))\n",
    "#emoji_arr[0, 0, emoji_idx[emoji]] = 1\n",
    "\n",
    "\n",
    "decode_sequence (emoji_to_oh(emoji, emoji_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â© T [VID] 181023 - Foi adicionada a letra D no outdoor misterioso do #BTS em Hollywood.Formando: BTS AND...  ILOVEPAR\n",
      "\n",
      "ğŸ’« T 181023 Kris Wu Studio update (3/3)Legendary creator at The Next Top Bang press conference #KrisWu #Wuyifa\n",
      "\n",
      "ğŸ˜  tho gha  apes pinging bay uthresthert hiter ood .co int sen che bostpe whth igh top and chonders Athtis ght. o/It.co/itthins \n",
      "\n",
      "ğŸ’› tsto/re belips \n",
      "\n",
      "ğŸ˜© ol thes man I whet ill of  houp s atpder hen. Iush thes herties aod ohe pire hhare pear.co/ht.Or I\n",
      "\n",
      "ğŸ‘‘ ar your folow ever you he als \n",
      "\n",
      "ğŸ¤©  Time  Epppaps h thricek deblk \n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ’• T Happy 23rd Birthday to Duckie Thot  https://t.co/pae7x6tfHz\n",
      "\n",
      "ğŸ˜±  Incredible tor invorive in ite iad a pop thime  hat se tcol 4ever un  ithas. \n",
      "\n",
      "ğŸ˜¢ T Toou hee lles #BT MPNi #BTS Met thest ane #BTOr #TM #MPARMYPhe tis  https://t.co/In99zRKwab\n",
      "\n",
      "ğŸ”¥ ored you hitp ia \n",
      "\n",
      "ğŸ”— T [DEPASCAL] RUSSEL HOODIE BLUE https://t.co/CR4QrnCiLn https://t.co/TgPX6G9r0k\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ‘‘ ar your folow ever you he als \n",
      "\n",
      "ğŸ™„ omeone didt ne ad to ritter sto Ehet pes hthess bolk hhtps://t.co/UtRCRYwe\n",
      "\n",
      "ğŸ’• T Happy 23rd Birthday to Duckie Thot  https://t.co/pae7x6tfHz\n",
      "\n",
      "ğŸ’“  folk overey noche wal to stome and on ties thams \n",
      "\n",
      "ğŸ˜±  Incredible tor invorive in ite iad a pop thime  hat se tcol 4ever un  ithas. \n",
      "\n",
      "ğŸ™ƒ  What ere wang Io handim allide neres hatpr ing to eme. of hetps://t.co/SU0gjA7rI \n",
      "\n",
      "ğŸ¤”  do thar ive lol ite doup hhtpy sardt hertone thtres theps IrStcon ans cous this frarihe areashe ill Ih thay sard thor thers ther Iht res..co/Ses!!\n",
      "\n",
      "ğŸ”¥ ored you hitp ia \n",
      "\n",
      "ğŸ’¦ T Meytisi liwan  ive rough time ind  ouple watch the so lole Ohe verd chat heops erolegone ereiching pops my lise \n",
      "\n",
      "ğŸ”— T [DEPASCAL] RUSSEL HOODIE BLUE https://t.co/CR4QrnCiLn https://t.co/TgPX6G9r0k\n",
      "\n",
      "ğŸ˜­ osEReday cooke S om #ThSimely Ifohthity lowethe rttps:/t.co/pUDbe7ho\n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ˜„ ost erelle ened so ou happes allee ee!!! \n",
      "\n",
      "ğŸ˜ T Big tiin  fosthe All whe pesserelig, SOpW bicces het re mancer ioth ancem tall who  popsper chanes reoch thtps://t.co/IU8AzRwaF \n",
      "\n",
      "ğŸ’¯  foothrip wath het maI think sop ihe rit are ther I I thies.. Plites ant pisht are are rore thers I ARTCRMYI N\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "â¡ T 2015  2016  2017  2018Im happy that youre my idol and that i can be with you through years#PrettyJeongyeon #YESo\n",
      "\n",
      "ğŸ˜‰  ahthe  alk you dare all oo hear so pich nos ertchest hops \n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ”¥ ored you hitp ia \n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ’‹  Thou  hat semand boou Soureresigh iteplia  happling hat semeclith themin waop hore mer things \n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ˜­ osEReday cooke S om #ThSimely Ifohthity lowethe rttps:/t.co/pUDbe7ho\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "â™¥ appy birth day thor 3eswT\n",
      "\n",
      "â˜º T # #JINWOO #JINU #WINNER   # 180913 one month ago https://t.co/j56C2iJle4\n",
      "\n",
      "ğŸ˜…  erter a vite  Imp fer ichans  pophe har thes .elo hetpin gatpes//t.co/SU09zRwa\n",
      "\n",
      "ğŸ‘‘ ar your folow ever you he als \n",
      "\n",
      "ğŸ¤£ T het rea theeld cind beist ill you ray tou hao goo pentire don. #10 Allys #\n",
      "\n",
      "ğŸ˜ T hets ron hever I mand ilis an miman lide hang souther ththtps I thops, neppinng sons yrerihe thit s I thiok \n",
      "\n",
      "ğŸ¤£ T het rea theeld cind beist ill you ray tou hao goo pentire don. #10 Allys #\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ¤”  do thar ive lol ite doup hhtpy sardt hertone thtres theps IrStcon ans cous this frarihe areashe ill Ih thay sard thor thers ther Iht res..co/Ses!!\n",
      "\n",
      "ğŸ˜¤ T the trasT Theppe://t.co/Isinelestrips ar hetre I thing \n",
      "\n",
      "ğŸ–¤  intie  tits s/t.co/SU9uvrever tose thaik I Sho my.OPr httss://t.co/SU09A7rrt\n",
      "\n",
      "ğŸ˜­ osEReday cooke S om #ThSimely Ifohthity lowethe rttps:/t.co/pUDbe7ho\n",
      "\n",
      "ğŸ˜ T hets ron hever I mand ilis an miman lide hang souther ththtps I thops, neppinng sons yrerihe thit s I thiok \n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ˜˜ igh tapk for richines lith tha bist postherr ghtres Indcohetperimgan pesing il  hetps:/tthe ghttps: \n",
      "\n",
      "â€¼ T STER RTe y t st nt at aret Tet ith top httpi:/tthellugetweat itht Zer coune wortthe gotps #R RTresWune salt https:/tthe/gstAV6 \n",
      "\n",
      "âœ¨  het pe alis en emi fad  ath smas Lil shim !! pelod tha tpre mone besinh tpis frackin  eer mealh treis go pester chasp porthe bol. hetrs://t.co/USARARwa #t\n",
      "\n",
      "ğŸ˜˜ igh tapk for richines lith tha bist postherr ghtres Indcohetperimgan pesing il  hetps:/tthe ghttps: \n",
      "\n",
      "ğŸ–¤  intie  tits s/t.co/SU9uvrever tose thaik I Sho my.OPr httss://t.co/SU09A7rrt\n",
      "\n",
      "ğŸƒ  happ  hatple hent posthice \n",
      "\n",
      "ğŸ’‹  Thou  hat semand boou Soureresigh iteplia  happling hat semeclith themin waop hore mer things \n",
      "\n",
      "ğŸ™ƒ  What ere wang Io handim allide neres hatpr ing to eme. of hetps://t.co/SU0gjA7rI \n",
      "\n",
      "ğŸ˜ T hets ron hever I mand ilis an miman lide hang souther ththtps I thops, neppinng sons yrerihe thit s I thiok \n",
      "\n",
      "ğŸ‰ ppy birthday pal, hope brides s poiled ye  x\n",
      "\n",
      "ğŸ’”  Ithe allowe and tough tous with ing out hou therst Bis , &amp; 5K Likes! Got starethen sie!!!\n",
      "\n",
      "ğŸ˜±  Incredible tor invorive in ite iad a pop thime  hat se tcol 4ever un  ithas. \n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ–¤  intie  tits s/t.co/SU9uvrever tose thaik I Sho my.OPr httss://t.co/SU09A7rrt\n",
      "\n",
      "ğŸ‘ T hetps shat henspen inges min s an meas lime aid to ming lou ht mes and OP hotppes thrthes Ablos #herryere hthrise ghops nteplis https://t.co/1UtRC2VJGc\n",
      "\n",
      "ğŸ’š T hetps shat henspen lige me as hatp sant lis nou hetr so and Zer I An thes ther hite! plo det ire AYou  tis thers hatps ant chet the st.cil the pepp hhapples ar\n",
      "ğŸ˜­ osEReday cooke S om #ThSimely Ifohthity lowethe rttps:/t.co/pUDbe7ho\n",
      "\n",
      "âœ… T hetps salD whetps://t.co/gessibe7 \n",
      "\n",
      "ğŸ˜ T hets ron hever I mand ilis an miman lide hang souther ththtps I thops, neppinng sons yrerihe thit s I thiok \n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ˜ T hets ron hever I mand ilis an miman lide hang souther ththtps I thops, neppinng sons yrerihe thit s I thiok \n",
      "\n",
      "ğŸ˜±  Incredible tor invorive in ite iad a pop thime  hat se tcol 4ever un  ithas. \n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ˜¢ T Toou hee lles #BT MPNi #BTS Met thest ane #BTOr #TM #MPARMYPhe tis  https://t.co/In99zRKwab\n",
      "\n",
      "ğŸ™ T hetps s/ath fereno fert he t al I \n",
      "\n",
      "ğŸ˜ª T hetps sal thestpeniggt pamke pharg sall yof aretour thert as I pT hant ereth hatps artthing so\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ’¥ T hetp s aht heno heverd me anpe ming s aping fom ther she thro/gho per ith s onghe me llieg one yor an ine so it sh thops thay sher. olthtis fans \n",
      "\n",
      "ğŸ˜” T hetps shat henspen ligg me I shit lal you hevoug \n",
      "\n",
      "ğŸŒ¸ T hetps shat hentper thtps I tall wen  hevem il ane panp hang send Nof hetrore and iee!. \n",
      "\n",
      "ğŸ˜±  Incredible tor invorive in ite iad a pop thime  hat se tcol 4ever un  ithas. \n",
      "\n",
      "ğŸ’¯  foothrip wath het maI think sop ihe rit are ther I I thies.. Plites ant pisht are are rore thers I ARTCRMYI N\n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ˜† T hetps shat henspen ligge mim y aht amma lerere the so lo ghe ? I Ahtd.. /I.. Routhins \n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n",
      "ğŸ˜±  Incredible tor invorive in ite iad a pop thime  hat se tcol 4ever un  ithas. \n",
      "\n",
      "ğŸ‘‹ T hatp s abch hen? redtpe /at. I Line tou her thess Oll I #Mie #lies. Poltheo ther ghet .o lith Oof Ithem.. Bfosero thttis /ttoo/z13w, hf tpses  hatps ht.ciss #H\n",
      "ğŸ’« T 181023 Kris Wu Studio update (3/3)Legendary creator at The Next Top Bang press conference #KrisWu #Wuyifa\n",
      "\n",
      "ğŸ¤£ T het rea theeld cind beist ill you ray tou hao goo pentire don. #10 Allys #\n",
      "\n",
      "ğŸ˜‡ T hetps aret hetps://t.co/igSX6 #\n",
      "\n",
      "ğŸ˜‚ he joys of having  amkied  ur tamist you ray! \n",
      "\n",
      "ğŸ˜‡ T hetps aret hetps://t.co/igSX6 #\n",
      "\n",
      "ğŸ’– T hetps shat henspenligg me ame aht saml ale want iod aol ith ams no dor not resthr thes po her onges \n",
      "\n",
      "â¤ T Heilk  fortht me and I ihin eleyo fa hitrin  tome bo wing on peming alk no  hetrs://t.co/UDf7h3oD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (100):\n",
    "    emoji = tweets_train.iloc[i].loc['emoji']\n",
    "    generated_tweet = decode_sequence (emoji_to_oh(emoji, emoji_idx))\n",
    "    print (emoji, generated_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "5 - seq2seq tweet generation model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
