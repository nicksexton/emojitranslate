{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# A Seq2seq model for generating tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_load_seq2seq_utils as s2s_util\n",
    "import data_load_utils as util\n",
    "from importlib import reload\n",
    "\n",
    "util = reload(util)\n",
    "s2s_util = reload(s2s_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tweets = util.filter_tweets_min_count(util.read_tweet_data('data/emojis_homemade.csv'), min_count=1000)\n",
    "tweets['text'] = util.filter_text_for_handles(tweets['text'])\n",
    "\n",
    "# After the filtering, remember to append a \\n character to each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT [VID] 181023 - Foi adicionada a letra D no ...</td>\n",
       "      <td>©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT 181023 Kris Wu Studio update (3/3)Legendary...</td>\n",
       "      <td>💫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT Now you are watching Indian SuperStar with ...</td>\n",
       "      <td>😎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dats for keeps</td>\n",
       "      <td>💛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Holy shit no I think.</td>\n",
       "      <td>😩</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emoji\n",
       "0  RT [VID] 181023 - Foi adicionada a letra D no ...     ©\n",
       "1  RT 181023 Kris Wu Studio update (3/3)Legendary...     💫\n",
       "2  RT Now you are watching Indian SuperStar with ...     😎\n",
       "3                                    dats for keeps      💛\n",
       "6                             Holy shit no I think.      😩"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445129, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Define the set of characters that we'll use to encode our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create dicts for character/emoji to index conversion\n",
    "\n",
    "chars_univ, chars_univ_idx = s2s_util.get_universal_chars_list()\n",
    "\n",
    "emojis = sorted(list(set(tweets['emoji'])))\n",
    "emoji_idx = dict((emoji, emojis.index(emoji)) for emoji in emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train_batches: 192.0\n",
      "n_dev_batches: 4.0\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = 2**18 + 2**17 # 8192  try 131072 = 2**18 for production\n",
    "DEV_SIZE = 2**13   # 8192  try 8192 = 2**13 for production\n",
    "\n",
    "TWEETS_PER_BATCH = 2048\n",
    "MAX_TWEET_LENGTH = 160\n",
    "n_train_batches = TRAIN_SIZE / TWEETS_PER_BATCH\n",
    "n_dev_batches = DEV_SIZE / TWEETS_PER_BATCH\n",
    "\n",
    "print (\"n_train_batches:\", n_train_batches)\n",
    "print (\"n_dev_batches:\", n_dev_batches)\n",
    "\n",
    "tweets_train = tweets.iloc[0:TRAIN_SIZE] # 8192 = 2**13\n",
    "tweets_dev = tweets.iloc[TRAIN_SIZE:TRAIN_SIZE+DEV_SIZE] # 2048 = 2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393216, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = s2s_util.xy_generator(tweets_train, emoji_indices=emoji_idx)\n",
    "dev_generator = s2s_util.xy_generator(tweets_dev, emoji_indices=emoji_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 161, 94)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([emoj, x], y) = train_generator.__next__()\n",
    "#e = emoj.reshape(64, 1, 111)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we're going to use the algorithm from the Keras example of a seq2seq model.\n",
    "We'll supply the emoji to the encoder LSTM which will encode it into two state vectors,\n",
    "and the decoder LSTM will be trained on the tweets using teacher forcing.\n",
    "\n",
    "\n",
    "\n",
    "# Summary of the algorithm\n",
    "\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "ENCODER_HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, len(emoji_idx)))\n",
    "encoder = LSTM(ENCODER_HIDDEN_SIZE, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, len(chars_univ)))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(ENCODER_HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(chars_univ), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None, 111)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, None, 94)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, 256), (None, 376832      input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, None, 256),  359424      input_18[0][0]                   \n",
      "                                                                 lstm_15[0][1]                    \n",
      "                                                                 lstm_15[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 94)     24158       lstm_16[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 760,414\n",
      "Trainable params: 760,414\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "192/192 [==============================] - 23s 118ms/step - loss: 1.5046 - val_loss: 1.2750\n",
      "Epoch 2/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 1.0662 - val_loss: 1.2530\n",
      "Epoch 3/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.6622 - val_loss: 1.5390\n",
      "Epoch 4/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.3145 - val_loss: 1.9416\n",
      "Epoch 5/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.1145 - val_loss: 2.2997\n",
      "Epoch 6/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0444 - val_loss: 2.6073\n",
      "Epoch 7/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0242 - val_loss: 2.7396\n",
      "Epoch 8/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0143 - val_loss: 2.9699\n",
      "Epoch 9/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0123 - val_loss: 3.0111\n",
      "Epoch 10/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0086 - val_loss: 3.1297\n",
      "Epoch 11/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0058 - val_loss: 3.2805\n",
      "Epoch 12/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0066 - val_loss: 3.1840\n",
      "Epoch 13/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0056 - val_loss: 3.2914\n",
      "Epoch 14/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0037 - val_loss: 3.4441\n",
      "Epoch 15/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0038 - val_loss: 3.4543\n",
      "Epoch 16/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0047 - val_loss: 3.4835\n",
      "Epoch 17/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0050 - val_loss: 3.4770\n",
      "Epoch 18/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0033 - val_loss: 3.5514\n",
      "Epoch 19/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0046 - val_loss: 3.5729\n",
      "Epoch 20/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0030 - val_loss: 3.6409\n",
      "Epoch 21/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0026 - val_loss: 3.7142\n",
      "Epoch 22/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0038 - val_loss: 3.6658\n",
      "Epoch 23/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0040 - val_loss: 3.6730\n",
      "Epoch 24/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0032 - val_loss: 3.6536\n",
      "Epoch 25/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0036 - val_loss: 3.7242\n",
      "Epoch 26/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0027 - val_loss: 3.7803\n",
      "Epoch 27/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0026 - val_loss: 3.7881\n",
      "Epoch 28/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0032 - val_loss: 3.7627\n",
      "Epoch 29/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0027 - val_loss: 3.7996\n",
      "Epoch 30/100\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.0027 - val_loss: 3.8114\n",
      "Epoch 31/100\n",
      "  1/192 [..............................] - ETA: 20s - loss: 0.0017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e09e1cb240b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_dev_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/emojitranslate-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=n_train_batches,\n",
    "                    epochs=100,\n",
    "                    validation_data=dev_generator,\n",
    "                    validation_steps=n_dev_batches,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save('emoji_s2s.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(ENCODER_HIDDEN_SIZE,))\n",
    "decoder_state_input_c = Input(shape=(ENCODER_HIDDEN_SIZE,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_chars_idx = dict(\n",
    "(i, char) for char, i in chars_univ_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(chars_univ)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, chars_univ_idx['\\n']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_chars_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > MAX_TWEET_LENGTH):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(chars_univ)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_oh(emoji, emoji_idx):\n",
    "    emoji_arr = np.zeros(shape=(1, 1, len(emoji_idx)))\n",
    "    emoji_arr[0, 0, emoji_idx[emoji]] = 1\n",
    "    return emoji_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😎\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Now you ar ywang Iis \\n'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make one hot vector for emoji input\n",
    "emoji = tweets_train.loc[2].loc['emoji']\n",
    "print (emoji)\n",
    "#emoji_arr = np.zeros(shape=(1, 1, len(emoji_idx)))\n",
    "#emoji_arr[0, 0, emoji_idx[emoji]] = 1\n",
    "\n",
    "\n",
    "decode_sequence (emoji_to_oh(emoji, emoji_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "© T [VID] 181023 - Foi adicionada a letra D no outdoor misterioso do #BTS em Hollywood.Formando: BTS AND...  ILOVEPAR\n",
      "\n",
      "💫 T181023 Kris Wu Studio update (3/3)Legendary creator at The Next Top Bang press conference #KrisWu #Wuyifa\n",
      "\n",
      "😎  Now you ar ywang Iis \n",
      "\n",
      "💛 ats for keeps \n",
      "\n",
      "😩 ol whe myo KrokedPla\n",
      "\n",
      "👑 T army, follow who retweet this \n",
      "\n",
      "🤩  Simpy bo-goo heaps:i/therabees. LOW #ie ifes icfese hothagh th ar stun  otes mong beos conde fermabling opsperoreadan ind. ifset hath thot het se st olus torden\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "💕 T Happy 23rd Birthday to Duckie Thot  https://t.co/pae7x6tfHz\n",
      "\n",
      "😱 T Incredible to be involved in the making of a mixtape that is now #1 on iTunes in 86 countries  https://t.co/Ii8AzRKwaF #RM\n",
      "\n",
      "😢 T Too much feels #BTS #MPN #BTSARMY https://t.co/OHK9zUT3bl\n",
      "\n",
      "🔥 oredol the poine changis  on ceust \n",
      "\n",
      "🔗 T [DEPASCAL] RUSSEL HOODIE BLUE https://t.co/CR4QrnCiLn https://t.co/TgPX6G9r0k\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "👑 T army, follow who retweet this \n",
      "\n",
      "🙄 omeone didng read be horige\n",
      "\n",
      "💕 T Happy 23rd Birthday to Duckie Thot  https://t.co/pae7x6tfHz\n",
      "\n",
      "💓  fol wever men the sothelas\n",
      "\n",
      "😱 T Incredible to be involved in the making of a mixtape that is now #1 on iTunes in 86 countries  https://t.co/Ii8AzRKwaF #RM\n",
      "\n",
      "🙃 WOW let me know when I can come play games \n",
      "\n",
      "🤔 e doot heap yats de chonee wal ghtpes all con eond tha s ut po/t.co InAbss \n",
      "\n",
      "🔥 oredol the poine changis  on ceust \n",
      "\n",
      "💦 T My fetish is to fa evemy ind at ie alliy as yas int rime wa tist ofa ghin py fo th ye ug to sedole linc  andise domb Suth ways fre thels ou ending domeb 'saps \n",
      "🔗 T [DEPASCAL] RUSSEL HOODIE BLUE https://t.co/CR4QrnCiLn https://t.co/TgPX6G9r0k\n",
      "\n",
      "😭 roset Folkes bo ghig  hal  heus hen go \n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "😄 ost people need cluSS !  Lots of them  !!   \n",
      "\n",
      "😁  Big Him  Kiss, bou ARMYs are faster. ht psalke p yan ind then ert shtt for the Zeol  oede Zer withengal en and no And tres th, fo ze, Oes Nered, ind .he plose  \n",
      "💯 T food tip with them@itzjncko https://t.co/OAZkYVOr8u\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "➡ T 2015  2016  2017  2018Im happy that youre my idol and that i can be with you through years#PrettyJeongyeon #YESo\n",
      "\n",
      "😉 T Whatever you think I got going on i do .. lmao \n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "🔥 oredol the poine changis  on ceust \n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "💋 T Like?  https://t.co/yeqJWBfmmQ\n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "😭 roset Folkes bo ghig  hal  heus hen go \n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "♥ appy birth day do coure wey \n",
      "\n",
      "☺ T # #JINWOO #JINU #WINNER   # 180913 one month ago https://t.co/j56C2iJle4\n",
      "\n",
      "😅 e Hore  aT the I came .o  amsy plit heving mo netps://t.co/faveryV18u\n",
      "\n",
      "👑 T army, follow who retweet this \n",
      "\n",
      "🤣 T She gotta be tanking about nastes Tos #Be MPle. ise all gh tpla/t.ch tJyJaysDanf fore tha ghet as th th ofade Neo soud yo all the sa heth and Lon Sor ens lfor \n",
      "😍 T Its #BurroBrunch time!! And that means LOTS of iced coffee horchata  booze, no booze, for here or to-go, SO. MANY. OP\n",
      "\n",
      "🤣 T She gotta be tanking about nastes Tos #Be MPle. ise all gh tpla/t.ch tJyJaysDanf fore tha ghet as th th ofade Neo soud yo all the sa heth and Lon Sor ens lfor \n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "🤔 e doot heap yats de chonee wal ghtpes all con eond tha s ut po/t.co InAbss \n",
      "\n",
      "😤 I thin  out hor youn wan ind se mar hutp ://t.co/LiZgS3oco\n",
      "\n",
      "🖤 oingime\n",
      "\n",
      "😭 roset Folkes bo ghig  hal  heus hen go \n",
      "\n",
      "😍 T Its #BurroBrunch time!! And that means LOTS of iced coffee horchata  booze, no booze, for here or to-go, SO. MANY. OP\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "😘 igh bouch fror chaty bas thy urD te are cong to manken.  hatps://t.co/1Ut9rery f\n",
      "\n",
      "‼ T STD FREE , bet yall wont retweet this\n",
      "\n",
      "✨ T Sending positive vibrations your way!! Please accept them and send to others \n",
      "\n",
      "😘 igh bouch fror chaty bas thy urD te are cong to manken.  hatps://t.co/1Ut9rery f\n",
      "\n",
      "🖤 oingime\n",
      "\n",
      "🎃 happy barthesp yor ther you whet yo  thank I dos hetps://t.co/gQVsiV6 8M\n",
      "\n",
      "💋 T Like?  https://t.co/yeqJWBfmmQ\n",
      "\n",
      "🙃 WOW let me know when I can come play games \n",
      "\n",
      "😍 T Its #BurroBrunch time!! And that means LOTS of iced coffee horchata  booze, no booze, for here or to-go, SO. MANY. OP\n",
      "\n",
      "🎉 appy birthdy p al, hope bredgs s poied ye  x\n",
      "\n",
      "💔 T I fee so attheckude beast Thend mol geves man Itdo manthes thos the fol ged ye ale tho retaes Thong #E o\n",
      "\n",
      "😱 T Incredible to be involved in the making of a mixtape that is now #1 on iTunes in 86 countries  https://t.co/Ii8AzRKwaF #RM\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "🖤 oingime\n",
      "\n",
      "👍 T heaply batr hed to u hteys://t.co/7qUnJHzf\n",
      "\n",
      "💚 T heap yorather youd hery NKws #PoW \n",
      "\n",
      "😭 roset Folkes bo ghig  hal  heus hen go \n",
      "\n",
      "✅ T heap yorather youd wer yils way #hop alketh ly han  and tol fen wil  thesme ledochin epstice nas Tn coude Arest het sal to for tourestire Souph thrys for thens\n",
      "😍 T Its #BurroBrunch time!! And that means LOTS of iced coffee horchata  booze, no booze, for here or to-go, SO. MANY. OP\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "😍 T Its #BurroBrunch time!! And that means LOTS of iced coffee horchata  booze, no booze, for here or to-go, SO. MANY. OP\n",
      "\n",
      "😱 T Incredible to be involved in the making of a mixtape that is now #1 on iTunes in 86 countries  https://t.co/Ii8AzRKwaF #RM\n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "😢 T Too much feels #BTS #MPN #BTSARMY https://t.co/OHK9zUT3bl\n",
      "\n",
      "🙏 T heap yobather you heryuy sals your went ro gemon ind you want  httshe/thol wowe #1od ino ereuth \n",
      "\n",
      "😪 T heap yobather don herpyou nhey#Pri#BeS Milet hats ://t.co/7qUnh57x\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "💥 T heap yorather youd whey Now wyou #Wol you \n",
      "\n",
      "😔 T heap yobather you herty://t.co/lUe7xbNz\n",
      "\n",
      "😱 T Incredible to be involved in the making of a mixtape that is now #1 on iTunes in 86 countries  https://t.co/Ii8AzRKwaF #RM\n",
      "\n",
      "💯 T food tip with them@itzjncko https://t.co/OAZkYVOr8u\n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "😆 T heap yorather youd wer tis sway gol an  othesom bet.shot peas # coryeferebich thopse theas nocp \n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "😱 T Incredible to be involved in the making of a mixtape that is now #1 on iTunes in 86 countries  https://t.co/Ii8AzRKwaF #RM\n",
      "\n",
      "💫 T181023 Kris Wu Studio update (3/3)Legendary creator at The Next Top Bang press conference #KrisWu #Wuyifa\n",
      "\n",
      "🤣 T She gotta be tanking about nastes Tos #Be MPle. ise all gh tpla/t.ch tJyJaysDanf fore tha ghet as th th ofade Neo soud yo all the sa heth and Lon Sor ens lfor \n",
      "😇 T heap yorather youd hery NKws #PoXP #Pe cond the s al chet rou htery foud Bon Sems!\n",
      "\n",
      "😂 he real reason I stopped watching #TheWalkingDead.  https://t.co/NG1gXYRJyu\n",
      "\n",
      "😇 T heap yorather youd hery NKws #PoXP #Pe cond the s al chet rou htery foud Bon Sems!\n",
      "\n",
      "💖 T heap yorather youd whey Now wyou #Wol you \n",
      "\n",
      "❤ T Love u girl, thanks for changing my life  https://t.co/Cu4FjW2emC\n",
      "\n",
      "💚 T heap yorather youd hery NKws #PoW \n",
      "\n",
      "😍 T Its #BurroBrunch time!! And that means LOTS of iced coffee horchata  booze, no booze, for here or to-go, SO. MANY. OP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (100):\n",
    "    emoji = tweets_train.iloc[i].loc['emoji']\n",
    "    generated_tweet = decode_sequence (emoji_to_oh(emoji, emoji_idx))\n",
    "    print (emoji, generated_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "5 - seq2seq tweet generation model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
