{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Part 2 - Training the model generate text jointly from emojis and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Following the success in getting the model to generate text based on training it on twitter data, we're going to modify the model to generate text based on two inputs - the emoji data and the text data.\n",
    "\n",
    "While the text will still be fed character-by-character into the LSTM, we're going to add an additional Dense input, which is just an n_emoji dimensional vector, where n_emoji is the number of possible emojis that we have sufficient data for (ie., more than 1000 training examples). The emojis will be one-hot encoded.\n",
    "\n",
    "First, we need to modify the data_load_utils files so that `convert_to_xy` gives us both one-hot encodings (text and emoji)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_load_utils as util\n",
    "from math import ceil\n",
    "\n",
    "from importlib import reload\n",
    "util = reload (util)\n",
    "\n",
    "# for cpu and memory profiling\n",
    "#%load_ext line_profiler\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's filter the data down to only emojis with more than 1000 training examples, and clean up the text by filtering out twitter handles, as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tweets = util.filter_tweets_min_count(util.read_tweet_data('data/emojis_homemade.csv'), min_count=1000)\n",
    "\n",
    "tweets['text'] = util.filter_text_for_handles(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445129, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First, let's get a list of all the emojis present in the tweets['emojis'] data series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TWEET_LENGTH = 160\n",
    "WINDOW_SIZE = 64\n",
    "STEP = 3\n",
    "\n",
    "samples_per_tweet = int(ceil((MAX_TWEET_LENGTH - WINDOW_SIZE) / STEP)) # 32\n",
    "tweets_per_batch = 2 #64\n",
    "samples_per_batch = samples_per_tweet * tweets_per_batch # 2048\n",
    "\n",
    "chars_univ, chars_univ_idx = util.get_universal_chars_list()\n",
    "\n",
    "emoji_series = tweets['emoji']\n",
    "emojis, emoji_idx = util.get_emojis_list(emoji_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# for prototyping\n",
    "#TRAIN_SIZE = 2**12\n",
    "#DEV_SIZE = 2**10\n",
    "\n",
    "TRAIN_SIZE = 2**18 # 32,768  try 131072 = 2**17 for production\n",
    "DEV_SIZE = 2**12   # 8192  try 8192 = 2**13 for production\n",
    "\n",
    "n_train_batches = TRAIN_SIZE / tweets_per_batch\n",
    "n_dev_batches = DEV_SIZE / tweets_per_batch\n",
    "\n",
    "tweets_train = tweets.iloc[0:TRAIN_SIZE] # 8192 = 2**13\n",
    "tweets_dev = tweets.iloc[TRAIN_SIZE:TRAIN_SIZE+DEV_SIZE] # 2048 = 2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 64 tweets x 32 samples per tweet = 2048 training examples per batch\n",
    "train_generator = util.convert_tweet_to_xy_generator(tweets_train, length=MAX_TWEET_LENGTH, \\\n",
    "                                                     window_size=WINDOW_SIZE,step=STEP, \\\n",
    "                                                     batch_size=tweets_per_batch, emoji_set=emojis)\n",
    "\n",
    "dev_generator = util.convert_tweet_to_xy_generator(tweets_dev, length=MAX_TWEET_LENGTH, \\\n",
    "                                                   window_size=WINDOW_SIZE,step=STEP, \\\n",
    "                                                   batch_size=tweets_per_batch, emoji_set = emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Building a network\n",
    "Intially, let's try generating tweets by training a network on just the tweet data. Once we have an idea how well we can get a network to generate tweets (remember, character by character), we'll compare it to a network that learns to generate tweets by predicting the next chracter jointly from the preceding text and an overall emoji. (remember, this dataset is tweets that all contain exactly one emoji)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Simple network - a single LSTM into a Dense softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_text shape: (64, 64, 93)\n",
      "x_emoji shape: (64, 111)\n",
      "y shape: (64, 93)\n"
     ]
    }
   ],
   "source": [
    "# Establish the shapes of the inputs/outputs\n",
    "([x_text, x_emoji], y) = next(train_generator)\n",
    "print (\"x_text shape:\", x_text.shape)\n",
    "print (\"x_emoji shape:\", x_emoji.shape)\n",
    "print (\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "x_text: 2048 x 64 x 93 (batch_size x window_size x len(chars_univ))\n",
    "x_emoji 2048 x 112 (batch_size x chars_univ)\n",
    "y 2048 x 93 (batch_size x chars_univ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, Input\n",
    "from keras.models import Model\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_input (InputLayer)         (None, 64, 93)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emoji_input (InputLayer)        (None, 112)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_lstm (LSTM)                (None, 256)          358400      text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           5650        emoji_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 306)          0           text_lstm[0][0]                  \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 93)           28551       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 392,601\n",
      "Trainable params: 392,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input = Input(shape=(WINDOW_SIZE, len(chars_univ)), dtype='float32', name='text_input')\n",
    "lstm = layers.LSTM(256, name='text_lstm')(text_input)\n",
    "\n",
    "emoji_input = Input(shape=(len(emojis),),\n",
    "                    dtype='float32',\n",
    "                    name='emoji_input')\n",
    "emoji_dense = layers.Dense (50, activation='relu')(emoji_input)\n",
    "\n",
    "concatenate = layers.concatenate([lstm, emoji_dense], name='concatenate')\n",
    "\n",
    "output = layers.Dense(len(chars_univ), name='output',\n",
    "                      activation='softmax')(concatenate)\n",
    "\n",
    "model = Model([text_input, emoji_input], output)\n",
    "model.compile(optimizer = keras.optimizers.RMSprop(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# can use this saved file for transfer learning\n",
    "# model = keras.models.load_model(\"models/tweet_gen_model-0.776.hdf5\") # 256 LSTM units, ~30 epochs training  # \n",
    "\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(layers.LSTM(256, input_shape=(WINDOW_SIZE, len(chars_univ)))) # was 128 units\n",
    "#model.add(layers.Dense(len(chars_univ), activation='softmax'))\n",
    "\n",
    "# loss function - targets are one-hot encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's see if we can use transfer learning to save a bit of time, by re-using the trained weights from the text-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tweet_only_model = keras.models.load_model(\"models/text_gen_model-train250k-0.830.hdf5\")\n",
    "# 256 LSTM units, ~30 epochs training\n",
    "\n",
    "tweet_model_weights = tweet_only_model.get_layer(index=0).get_weights()\n",
    "current_model_lstm_layer = model.get_layer(name='text_lstm')\n",
    "current_model_lstm_layer.set_weights(tweet_model_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training the model and sampling from it using a standard character-by-character method\n",
    "1. Draw a probability distribution for the next character\n",
    "2. Reweight the distribution using a temperature parameter\n",
    "3. Sample the next character at random using the reweighted distribution\n",
    "4. Add the new character at the end of the available list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sample (preds, temperature = 1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## train the model, generate text\n",
    "Use a range of temeratures after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT [VID] 1'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/text_emoji_joint_gen_model-0.840.hdf5\") # resume progress, 10+2+1 epoch completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 543ms/step - loss: 0.3892 - acc: 0.9402 - val_loss: 1.2701 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27012, saving model to text_emoji_joint_gen_model-0.389.hdf5\n",
      "--- Generating with seed: \"RT I was fighting with my nigga one day in my house and I told h\" | ðŸ˜­\n",
      "--------- temperature: 0.3\n",
      "RT I was fighting with my nigga one day in my house and I told his this ag this way this                                                                        \n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT I was fighting with my nigga one day in my house and I told his the retwott the my how thas I ave this hat by the soud this seostiouf  hatisid this  happy th\n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT I was fighting with my nigga one day in my house and I told has #bythasingranenot   Re watt as you and readiti they 4o real lit rettr  stintre the reagestzmy\n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT I was fighting with my nigga one day in my house and I told hop that Rithass Gass,  o  i li bade this timer is this mythong wood hithhser P! RT Got  oh shas \n",
      "\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 539ms/step - loss: 0.3622 - acc: 0.9495 - val_loss: 1.2682 - val_acc: 0.7207\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.27012 to 1.26825, saving model to text_emoji_joint_gen_model-0.362.hdf5\n",
      "--- Generating with seed: \"RT This fake ass fuck lmaoooo  the troll levels https://t.co/3xZ\" | ðŸ’€\n",
      "--------- temperature: 0.3\n",
      "RT This fake ass fuck lmaoooo  the troll levels https://t.co/3xZhhhiSs##IneancMenciseryanclass ##IN #Manchaspingelsinsingane  #and the Weat sands in the aasine \n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT This fake ass fuck lmaoooo  the troll levels https://t.co/3xZhhhrkstthitwnoxtod Tha camping moxinpes wanting the reme tay Frem happy this ther woth this fall\n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT This fake ass fuck lmaoooo  the troll levels https://t.co/3xZhtho3l3incuhOntyBy ack necorde an isali. no Is dichs Ug with and sount this lideryy much trenl  \n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT This fake ass fuck lmaoooo  the troll levels https://t.co/3xZhhRw3l3 #M-SimmawansRnmy on .  get steal and that   ank #this.. be 4y is and retritn and caacid \n",
      "\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 541ms/step - loss: 0.3411 - acc: 0.9546 - val_loss: 1.2706 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"Would someone be so kind as to flood my DMs and/or mentions with\" | ðŸ˜”\n",
      "--------- temperature: 0.3\n",
      "Would someone be so kind as to flood my DMs and/or mentions with the manci les is this fampemy.  a cam im the reme my happy...                                  \n",
      "\n",
      "--------- temperature: 0.5\n",
      "Would someone be so kind as to flood my DMs and/or mentions with this fammen fomeo   lees.  it I fol themren id now #Madnackinger Pascing samply theme way thin \n",
      "\n",
      "--------- temperature: 0.8\n",
      "Would someone be so kind as to flood my DMs and/or mentions with a mixcrd on thiunth age and Fontion I wan to as Aanione reme foreo this   am noc  im gith!     \n",
      "\n",
      "--------- temperature: 1.0\n",
      "Would someone be so kind as to flood my DMs and/or mentions with you wont fooo- this swollo io I thot sow  counne foreerverendoth has prypastcnit maci .. goteme\n",
      "\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 546ms/step - loss: 0.3220 - acc: 0.9565 - val_loss: 1.2801 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"RT You cant let all these people rattle you when you will only m\" | ðŸ˜‚\n",
      "--------- temperature: 0.3\n",
      "RT You cant let all these people rattle you when you will only making  hat and come this wanding                                                                \n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT You cant let all these people rattle you when you will only mant to cancioll stottren Dont woth this  as but song on hap  at  lasicgome this all son tom  b  \n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT You cant let all these people rattle you when you will only mant on woo feets themrane won is not #ound this was the made mexhame that campe tament anticle s\n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT You cant let all these people rattle you when you will only makiglis doy!!xell methens upt shit thcen..  happ.  this woull Wate this one by stotthess btthere\n",
      "\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 536ms/step - loss: 0.2981 - acc: 0.9626 - val_loss: 1.2802 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"RT The new mural by Phlegm in Hackney. The artist incorporates t\" | â¤\n",
      "--------- temperature: 0.3\n",
      "RT The new mural by Phlegm in Hackney. The artist incorporates the fereon   Reme the foreet the realey for the reme treng thremen  #famale for the foreon  pure \n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT The new mural by Phlegm in Hackney. The artist incorporates the for and realit frome  #pancistreng #andting no one pooong themrer weolled and retwing the for\n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT The new mural by Phlegm in Hackney. The artist incorporates the reamy.  os your won  !I for the reagio t  bere tore but .  happyy..me im goterevant trem #nou\n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT The new mural by Phlegm in Hackney. The artist incorporates theed Ar &appy themire A mittorry you no ada gwo thend thent for 1o for1, thry fore theer aMl ame\n",
      "\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 553ms/step - loss: 0.2855 - acc: 0.9646 - val_loss: 1.2871 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"                               RT shoot yall friendship shot pls\" | ðŸ˜©\n",
      "--------- temperature: 0.3\n",
      "                               RT shoot yall friendship shot pls sill the   RT IN  I Stoa this stithher whother well !                                          \n",
      "\n",
      "--------- temperature: 0.5\n",
      "                               RT shoot yall friendship shot plsis  pas in thit andile thit betting #pansthing my happy thit aly wou shonering   remlise   follo\n",
      "\n",
      "--------- temperature: 0.8\n",
      "                               RT shoot yall friendship shot plsos no done mont ion  Remaging the wotthe woth frrmus foree this thit agothey you woinn lis  hot \n",
      "\n",
      "--------- temperature: 1.0\n",
      "                               RT shoot yall friendship shot pls sime this not of byyhersto frasting   sterGI Lol cay get Shompely! LaRetimala ackfind but reoll\n",
      "\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 545ms/step - loss: 0.2661 - acc: 0.9675 - val_loss: 1.2874 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"                RT Im both happy and depressed at the same time \" | ðŸ˜­\n",
      "--------- temperature: 0.3\n",
      "                RT Im both happy and depressed at the same time this sime tire the mani  hessing the reasing the sindowhttpry this ass simeston you won this sti\n",
      "\n",
      "--------- temperature: 0.5\n",
      "                RT Im both happy and depressed at the same time lid soo   ot remasing the sone and re and the reasing this thisk you worth this stith reanis  ha\n",
      "\n",
      "--------- temperature: 0.8\n",
      "                RT Im both happy and depressed at the same time   hat recass.  hat the tryss gothsreent.  dostriouh.                                            \n",
      "\n",
      "--------- temperature: 1.0\n",
      "                RT Im both happy and depressed at the same time ass b residing  #Wastes shosthe  hat  the srertirgr stonifritherPasiiss,dont thank you who my Do\n",
      "\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 529ms/step - loss: 0.2396 - acc: 0.9753 - val_loss: 1.3143 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"       RT Retweet to gain bts Stan mutuals, follow who retweets \" | â¤\n",
      "--------- temperature: 0.3\n",
      "       RT Retweet to gain bts Stan mutuals, follow who retweets the foreen who   Rom #MynKiday happy.                                                           \n",
      "\n",
      "--------- temperature: 0.5\n",
      "       RT Retweet to gain bts Stan mutuals, follow who retweets thenouf the reaure the frement prentee the thot foreee you woen                                 \n",
      "\n",
      "--------- temperature: 0.8\n",
      "       RT Retweet to gain bts Stan mutuals, follow who retweets this ! # I KMSSSzLIth your repwy.                                                               \n",
      "\n",
      "--------- temperature: 1.0\n",
      "       RT Retweet to gain bts Stan mutuals, follow who retweets frmeE this aby  the ghaw avf 1bonsioe wot bat fommy myternoteen. dot foX wUf domy think you won \n",
      "\n",
      "epoch 9\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 524ms/step - loss: 0.2276 - acc: 0.9758 - val_loss: 1.3027 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"                                         RT wgats in da package \" | ðŸ‘€\n",
      "--------- temperature: 0.3\n",
      "                                         RT wgats in da package do this simel me be tirst them  the rememing proping themrere bothiss cam gothee wathit on for h\n",
      "\n",
      "--------- temperature: 0.5\n",
      "                                         RT wgats in da package this                                                                                            \n",
      "\n",
      "--------- temperature: 0.8\n",
      "                                         RT wgats in da package ver i  Pell Stagis up mixhim the redwiot doosine myth the andignt redest    RamY got reamifre re\n",
      "\n",
      "--------- temperature: 1.0\n",
      "                                         RT wgats in da package the am s langide thit real you worter Diggontuppy Rod campide amy 5 as real Love thrmaszhdis. Pe\n",
      "\n",
      "epoch 10\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 531ms/step - loss: 0.2121 - acc: 0.9788 - val_loss: 1.3090 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"RT Former and current cast members of #TheWalkingDead pay tribut\" | â¤\n",
      "--------- temperature: 0.3\n",
      "RT Former and current cast members of #TheWalkingDead pay tributh treare prentee them the remugr #panced the amy foree and retwert themrere and rede the reme th\n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT Former and current cast members of #TheWalkingDead pay tributhyt. and rememme reme the reno reat the fereon you won  30o  #fangime they make our gotere theyr\n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT Former and current cast members of #TheWalkingDead pay tributh trengion themrery.  theyre reat your reme thit foren #MTSSouy Koor eopretrengrep  in Im Dont A\n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT Former and current cast members of #TheWalkingDead pay tributn the a #frocrey repAited got the fanSed 1 on 1 fal!!  Rupt Pinnne paysent them is p.omee 13 me \n",
      "\n",
      "epoch 11\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 525ms/step - loss: 0.2048 - acc: 0.9780 - val_loss: 1.3138 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"  Thank you Bridget  love you back! When are we like meeting omg\" | â¤\n",
      "--------- temperature: 0.3\n",
      "  Thank you Bridget  love you back! When are we like meeting omg the redoid the myner  #pancheener #paypyKeya #paystion                                         \n",
      "\n",
      "--------- temperature: 0.5\n",
      "  Thank you Bridget  love you back! When are we like meeting omg ree you woo the reme the fam!  and reves in the retwoot                                        \n",
      "\n",
      "--------- temperature: 0.8\n",
      "  Thank you Bridget  love you back! When are we like meeting omg reauntre thin rerativeromy..  the realy for therre. #pangang the wone ti got my foreomer my bye\n",
      "\n",
      "--------- temperature: 1.0\n",
      "  Thank you Bridget  love you back! When are we like meeting omgher nother . up bourthreyrevrey my frrmm whot  A feywand theeny any Kow of Looder #yon eprytre a\n",
      "\n",
      "epoch 12\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 527ms/step - loss: 0.1879 - acc: 0.9807 - val_loss: 1.3389 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"RT Here is a video to brighten up ur timeline #AlwaysWithEXO htt\" | ðŸ’–\n",
      "--------- temperature: 0.3\n",
      "RT Here is a video to brighten up ur timeline #AlwaysWithEXO http::/t..oo/5trabrblplaksangically so sinstamy.  but Aucles cam got the assing the manic monthing \n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT Here is a video to brighten up ur timeline #AlwaysWithEXO http::/tt.oo/sarXXib#pT # #MT # #Thatappanging  #andingidilyasacksing passing this tick this gioss \n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT Here is a video to brighten up ur timeline #AlwaysWithEXO http ittttres this Ab pisstserites #hatcKuresol the a4d coung plyns the Oout them wat Joch gote and\n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT Here is a video to brighten up ur timeline #AlwaysWithEXO http::/t..ooohbaKjHllrpTampapid ack nampCYUU Frammige presssEngont 1 op  simext.. is cuck in Than y\n",
      "\n",
      "epoch 13\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 537ms/step - loss: 0.1725 - acc: 0.9832 - val_loss: 1.3476 - val_acc: 0.7207\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"                                       Im getting drunk already \" | ðŸ˜‚\n",
      "--------- temperature: 0.3\n",
      "                                       Im getting drunk already with the manicamplyshiss blass lot making  lot fallo wonth thin show the Ohas that  brent to wat\n",
      "\n",
      "--------- temperature: 0.5\n",
      "                                       Im getting drunk already with this stopping Ame and be happ my manicl dost on Some that sting Domd!  happy. not foreat an\n",
      "\n",
      "--------- temperature: 0.8\n",
      "                                       Im getting drunk already the aaside and and the masing I has bathlobs who wond the hates  thay wougs he was this   wo ce \n",
      "\n",
      "--------- temperature: 1.0\n",
      "                                       Im getting drunk already fakmo wont     me the wattoman  baksbcnamasd don people makhenss!  dos APny3k bush hap ant som t\n",
      "\n",
      "epoch 14\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 33s 523ms/step - loss: 0.1637 - acc: 0.9839 - val_loss: 1.3704 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"                                     Drive back from graduation \" | ðŸ¤£\n",
      "--------- temperature: 0.3\n",
      "                                     Drive back from graduation you wond you woun this  plasse wot happy thit  bat and rememter the maticre and come the woth th\n",
      "\n",
      "--------- temperature: 0.5\n",
      "                                     Drive back from graduation mo hat andie the woul a bat lith the reale that my hatttring better woth wot them wat but ..... \n",
      "\n",
      "--------- temperature: 0.8\n",
      "                                     Drive back from graduation you wond you when you woul Seeds byther  ane but telagiol ano be tim happuts it thiS  pam bett t\n",
      "\n",
      "--------- temperature: 1.0\n",
      "                                     Drive back from graduation   am simle bottreagime tel evanee  brt OH pays this stamy byde ment. tere doidg a bie.  pl wals \n",
      "\n",
      "epoch 15\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 548ms/step - loss: 0.1516 - acc: 0.9836 - val_loss: 1.3746 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"         RT Odell + \"Mo Bamba\" had Death Valley going crazy (via\" | ðŸ”¥\n",
      "--------- temperature: 0.3\n",
      "         RT Odell + \"Mo Bamba\" had Death Valley going crazy (viass samdes dostring  pass thit stittrr ssong in the real my hit therress on this sick this spicli\n",
      "\n",
      "--------- temperature: 0.5\n",
      "         RT Odell + \"Mo Bamba\" had Death Valley going crazy (viast on you woth time hit cremano store the realie the wottr the ass camchit the feck my hit the m\n",
      "\n",
      "--------- temperature: 0.8\n",
      "         RT Odell + \"Mo Bamba\" had Death Valley going crazy (viass not remindiow #Ransimy prests tremb this aps #prmangionshothas thitk bathiss #assiPanchers re\n",
      "\n",
      "--------- temperature: 1.0\n",
      "         RT Odell + \"Mo Bamba\" had Death Valley going crazy (viastsow pr ng distrongiyoPanccP  #RangcypasustJs #masingPaygissOmatidys dica noctring #WastendyKa#\n",
      "\n",
      "epoch 16\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 554ms/step - loss: 0.1442 - acc: 0.9841 - val_loss: 1.4004 - val_acc: 0.7139\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"     RT Any day all day, Ill choose this https://t.co/mYfoGBmW2R\" | ðŸ¤©\n",
      "--------- temperature: 0.3\n",
      "     RT Any day all day, Ill choose this https://t.co/mYfoGBmW2R low wand this samoth themren watching the really with thinksting  people watting whtther well d\n",
      "\n",
      "--------- temperature: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RT Any day all day, Ill choose this https://t.co/mYfoGBmW2RsaSlasSelaGe wored  leangalle we won thas thitkid happy.  reepe.  lell spolle  is camphitre wos \n",
      "\n",
      "--------- temperature: 0.8\n",
      "     RT Any day all day, Ill choose this https://t.co/mYfoGBmW2R #lassipaylisicagmape and copepates Oo happy Roddillied thingso.                                \n",
      "\n",
      "--------- temperature: 1.0\n",
      "     RT Any day all day, Ill choose this https://t.co/mYfoGBmW2RlackidgIIl AR 40 idancmenichPO wdNObe myching patslest angps you wow s as  leck simentt the myen\n",
      "\n",
      "epoch 17\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 532ms/step - loss: 0.1415 - acc: 0.9844 - val_loss: 1.3997 - val_acc: 0.7148\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"I love impractical jokers so much! I wish I had the confidence a\" | ðŸ˜‚\n",
      "--------- temperature: 0.3\n",
      "I love impractical jokers so much! I wish I had the confidence and sond the matill who  no dont won  loding with this shotts follo  all with this Some tho hatti\n",
      "\n",
      "--------- temperature: 0.5\n",
      "I love impractical jokers so much! I wish I had the confidence and shis that nock                                                                               \n",
      "\n",
      "--------- temperature: 0.8\n",
      "I love impractical jokers so much! I wish I had the confidence and fosphana bottic  me lof modhiagnod the retwent  they dont reat toid!  theyre and manci me thi\n",
      "\n",
      "--------- temperature: 1.0\n",
      "I love impractical jokers so much! I wish I had the confidence and sands! Deme thas what !leepsing thls with my happy.  e nevs thos myth can!!       RT im dingo\n",
      "\n",
      "epoch 18\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 34s 529ms/step - loss: 0.1326 - acc: 0.9863 - val_loss: 1.3903 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"  RT  if I saw it, yall gotta see it too https://t.co/UQsus1lyvm\" | â˜¹\n",
      "--------- temperature: 0.3\n",
      "  RT  if I saw it, yall gotta see it too https://t.co/UQsus1lyvm tavey a lot #prystanting the watthis and the hatts fother  be sount is this shot the hatter way\n",
      "\n",
      "--------- temperature: 0.5\n",
      "  RT  if I saw it, yall gotta see it too https://t.co/UQsus1lyvm thas by this stith and reep this follo dottring                                                \n",
      "\n",
      "--------- temperature: 0.8\n",
      "  RT  if I saw it, yall gotta see it too https://t.co/UQsus1lyvmStrangIngo Oh matcreday was thot. Happy themretwerted bethine and come they make tho woold  pang\n",
      "\n",
      "--------- temperature: 1.0\n",
      "  RT  if I saw it, yall gotta see it too https://t.co/UQsus1lyvm Oiffasth the Love and reel my fiNnteren of this fachamy Krome th theasityfring e mincasmostor w\n",
      "\n",
      "epoch 19\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 35s 552ms/step - loss: 0.1224 - acc: 0.9866 - val_loss: 1.3995 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.26825\n",
      "--- Generating with seed: \"RT BH9 lettuce farm please retweet for me  https://t.co/F5EhFkBu\" | ðŸ™\n",
      "--------- temperature: 0.3\n",
      "RT BH9 lettuce farm please retweet for me  https://t.co/F5EhFkBuorapaypeng pances on was this  as but sinsthis soon won the redwoo with the redes with this spol\n",
      "\n",
      "--------- temperature: 0.5\n",
      "RT BH9 lettuce farm please retweet for me  https://t.co/F5EhFkBurodaytick    Happy... is dont won this sandill  shit this spocl wooth you woun thin spolls reall\n",
      "\n",
      "--------- temperature: 0.8\n",
      "RT BH9 lettuce farm please retweet for me  https://t.co/F5EhFkBuorancHasicko is doung woll won  pancieng are is dofzne won thitw beee who wone dont wone really \n",
      "\n",
      "--------- temperature: 1.0\n",
      "RT BH9 lettuce farm please retweet for me  https://t.co/F5EhFkBuHpppyhappy... RT #LmastiMuchey #Thiarlevile h,7 keng wils this and Hope won prennioline nod and \n",
      "\n",
      "epoch 20\n",
      "Epoch 1/1\n",
      "61/64 [===========================>..] - ETA: 1s - loss: 0.1187 - acc: 0.9857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-137:\n",
      "Process ForkPoolWorker-136:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-203af4b18086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                          \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# run the generator in a separate thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                          )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "n_seed_chars = 64 # number of characters to use as a seed for text generation\n",
    "\n",
    "model.optimizer.lr.assign(0.001) # to reset the learning rate if running additional training\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath='text_emoji_joint_gen_model-{loss:.3f}.hdf5', \n",
    "                                       verbose=1, \n",
    "                                       save_best_only=True)\n",
    "\n",
    "# train for 60 epochs\n",
    "for epoch in range (1, 60):\n",
    "    print ('epoch', epoch)\n",
    "\n",
    "    # fit the model for one iteration\n",
    "    model.fit_generator (train_generator,\n",
    "                         steps_per_epoch=n_train_batches, # 64 x 32 = batches of 2048\n",
    "                         epochs=1,\n",
    "                         validation_data=dev_generator, \n",
    "                         validation_steps=n_dev_batches,\n",
    "                         callbacks=[checkpoint],\n",
    "                         verbose=1,\n",
    "                         use_multiprocessing=True, # run the generator in a separate thread\n",
    "                         )\n",
    "\n",
    "    # select a text seed at random\n",
    "    seed_tweet = tweets.iloc[random.randint(0, len(tweets))]\n",
    "    seed_text = util.pad_text(seed_tweet['text'][0:n_seed_chars], n_seed_chars)\n",
    "    generated_text = seed_text\n",
    "    #one-hot encode the emoji\n",
    "    emoji_one_hot = util.get_emoji_bool_array(seed_tweet['emoji'], emoji_idx)\n",
    "    print ('--- Generating with seed: \"' + generated_text + '\" | ' + seed_tweet['emoji'])\n",
    "\n",
    "    \n",
    "    # try a range of sampling temperatures\n",
    "    for temperature in [0.3, 0.5, 0.8, 1.0]:\n",
    "        generated_text = seed_text\n",
    "        print ('--------- temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        for i in range (MAX_TWEET_LENGTH - n_seed_chars):\n",
    "            # one-hot encode the characters generated so far\n",
    "            sampled = np.zeros((1, WINDOW_SIZE, len(chars_univ)))\n",
    "            for t, char in enumerate (generated_text):\n",
    "                sampled[0, t, chars_univ_idx[char]] = 1\n",
    "\n",
    "            # sample the next character\n",
    "            preds = model.predict([sampled, emoji_one_hot], verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars_univ[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "\n",
    "        print (\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random, sys\n",
    "def generate_tweet(seed_text, seed_emoji, emoji_idx, window_size=WINDOW_SIZE,\n",
    "\t\t   length=MAX_TWEET_LENGTH, temperatures=[0.3, 0.5, 0.8, 1.0],\n",
    "\t\t   random_seed = None):\n",
    "\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed) # np.random.seed needed by tf \n",
    "        tf.set_random_seed(random_seed)\n",
    "        \n",
    "\n",
    "    n_seed_chars = window_size\n",
    "    #first, pad out the seed_text with whitespace     \n",
    "    seed_text = util.pad_text(seed_text, window_size)\n",
    "    #one-hot encode the emoji\n",
    "    emoji_one_hot = util.get_emoji_bool_array(seed_emoji, emoji_idx)\n",
    "    print ('--- Generating with seed: \"' + seed_text + '\" [' + str(len(seed_text)) + ']')\n",
    "\n",
    "    # try a range of sampling temperatures\n",
    "    for temperature in temperatures:\n",
    "        generated_text = seed_text\n",
    "        print ('--------- temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        for i in range (length - n_seed_chars):\n",
    "            # one-hot encode the characters generated so far\n",
    "            sampled = np.zeros((1, WINDOW_SIZE, len(chars_univ)))\n",
    "            for t, char in enumerate (generated_text):\n",
    "                sampled[0, t, chars_univ_idx[char]] = 1\n",
    "\n",
    "            # sample the next character\n",
    "            preds = model.predict([sampled, emoji_one_hot], verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars_univ[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "\n",
    "        print (\"\\n\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_seed_text(tweets, seed_length=64, random_seed = None):\n",
    "    if random_seed: np.random.seed(random_seed)\n",
    "        \n",
    "    seed_tweet = tweets.iloc[np.random.randint(0, len(tweets))]\n",
    "    seed_text = seed_tweet['text'][0:seed_length]\n",
    "    seed_emoji = seed_tweet['emoji']\n",
    "    \n",
    "    print (seed_text + \"\\n\" + seed_tweet['text'] + \"|\" + seed_emoji)\n",
    "    \n",
    "    return seed_text, seed_emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omg youre sweet, thank you!\n",
      "Omg youre sweet, thank you!|â˜º\n"
     ]
    }
   ],
   "source": [
    "seed_text, seed_emoji = get_seed_text(tweets, seed_length=64, random_seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \"                                     Omg youre sweet, thank you!\" [64]\n",
      "--------- temperature: 0.8\n",
      "                                     Omg youre sweet, thank you!! I still be much for an important moment and shaie jumping for this for words, bettee  #lovestanding # #wins   # # # # # # #JIMIN# # # \n",
      "\n",
      "--------- temperature: 0.9\n",
      "                                     Omg youre sweet, thank you!!! and I did walking nothing out in an a spssss and shes left the lovery star private ruming her family  where the lucky                \n",
      "\n",
      "--------- temperature: 1.0\n",
      "                                     Omg youre sweet, thank you! And hake fucking Go done is not feel positivel!! Thanks to like that name I can come for my nigga  and she is more!!  @likeaselandim #s\n",
      "\n",
      "--------- temperature: 1.1\n",
      "                                     Omg youre sweet, thank you!  Gillurinura waste   A heall spagiuiting  &amp; wanting the ball a Happy thing abu  beforeah I have been a hair clinch Nidebs played us\n",
      "\n",
      "--------- temperature: 1.2\n",
      "                                     Omg youre sweet, thank you!!!King Fans dont well.  She a Whats#GhamCumbE_V  Y65 Lyjs Teg My X2 Jinues!  Yee who's lette: 10 Mples #BTS really cereny with 5 hJam bs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_tweet (seed_text, 'ðŸ’“', emoji_idx, length=200, temperatures=[0.8, 0.9, 1.0, 1.1, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Â©': 0,\n",
       " 'â€¼': 1,\n",
       " 'â–¶': 2,\n",
       " 'â˜¹': 3,\n",
       " 'â˜º': 4,\n",
       " 'â™¥': 5,\n",
       " 'âš ': 6,\n",
       " 'âš¡': 7,\n",
       " 'âœ…': 8,\n",
       " 'âœˆ': 9,\n",
       " 'âœŠ': 10,\n",
       " 'âœŒ': 11,\n",
       " 'âœ”': 12,\n",
       " 'âœ¨': 13,\n",
       " 'â—': 14,\n",
       " 'â£': 15,\n",
       " 'â¤': 16,\n",
       " 'âž¡': 17,\n",
       " 'â­': 18,\n",
       " 'ðŸŒŸ': 19,\n",
       " 'ðŸŒ¹': 20,\n",
       " 'ðŸŽƒ': 21,\n",
       " 'ðŸŽ‰': 22,\n",
       " 'ðŸŽ¶': 23,\n",
       " 'ðŸ†': 24,\n",
       " 'ðŸ‘€': 25,\n",
       " 'ðŸ‘‡': 26,\n",
       " 'ðŸ‘‰': 27,\n",
       " 'ðŸ‘Š': 28,\n",
       " 'ðŸ‘Œ': 29,\n",
       " 'ðŸ‘': 30,\n",
       " 'ðŸ‘': 31,\n",
       " 'ðŸ‘‘': 32,\n",
       " 'ðŸ’€': 33,\n",
       " 'ðŸ’‹': 34,\n",
       " 'ðŸ’“': 35,\n",
       " 'ðŸ’”': 36,\n",
       " 'ðŸ’•': 37,\n",
       " 'ðŸ’–': 38,\n",
       " 'ðŸ’—': 39,\n",
       " 'ðŸ’˜': 40,\n",
       " 'ðŸ’™': 41,\n",
       " 'ðŸ’š': 42,\n",
       " 'ðŸ’›': 43,\n",
       " 'ðŸ’œ': 44,\n",
       " 'ðŸ’ž': 45,\n",
       " 'ðŸ’¥': 46,\n",
       " 'ðŸ’¦': 47,\n",
       " 'ðŸ’ª': 48,\n",
       " 'ðŸ’«': 49,\n",
       " 'ðŸ’¯': 50,\n",
       " 'ðŸ“': 51,\n",
       " 'ðŸ“·': 52,\n",
       " 'ðŸ“¸': 53,\n",
       " 'ðŸ“½': 54,\n",
       " 'ðŸ”—': 55,\n",
       " 'ðŸ”¥': 56,\n",
       " 'ðŸ”´': 57,\n",
       " 'ðŸ–¤': 58,\n",
       " 'ðŸ—£': 59,\n",
       " 'ðŸ˜€': 60,\n",
       " 'ðŸ˜': 61,\n",
       " 'ðŸ˜‚': 62,\n",
       " 'ðŸ˜„': 63,\n",
       " 'ðŸ˜…': 64,\n",
       " 'ðŸ˜†': 65,\n",
       " 'ðŸ˜‡': 66,\n",
       " 'ðŸ˜ˆ': 67,\n",
       " 'ðŸ˜‰': 68,\n",
       " 'ðŸ˜Š': 69,\n",
       " 'ðŸ˜‹': 70,\n",
       " 'ðŸ˜Œ': 71,\n",
       " 'ðŸ˜': 72,\n",
       " 'ðŸ˜Ž': 73,\n",
       " 'ðŸ˜': 74,\n",
       " 'ðŸ˜': 75,\n",
       " 'ðŸ˜‘': 76,\n",
       " 'ðŸ˜’': 77,\n",
       " 'ðŸ˜”': 78,\n",
       " 'ðŸ˜•': 79,\n",
       " 'ðŸ˜˜': 80,\n",
       " 'ðŸ˜œ': 81,\n",
       " 'ðŸ˜ž': 82,\n",
       " 'ðŸ˜¡': 83,\n",
       " 'ðŸ˜¢': 84,\n",
       " 'ðŸ˜¤': 85,\n",
       " 'ðŸ˜©': 86,\n",
       " 'ðŸ˜ª': 87,\n",
       " 'ðŸ˜«': 88,\n",
       " 'ðŸ˜¬': 89,\n",
       " 'ðŸ˜­': 90,\n",
       " 'ðŸ˜±': 91,\n",
       " 'ðŸ˜³': 92,\n",
       " 'ðŸ˜´': 93,\n",
       " 'ðŸ™‚': 94,\n",
       " 'ðŸ™ƒ': 95,\n",
       " 'ðŸ™„': 96,\n",
       " 'ðŸ™ˆ': 97,\n",
       " 'ðŸ™Œ': 98,\n",
       " 'ðŸ™': 99,\n",
       " 'ðŸš¨': 100,\n",
       " 'ðŸ¤”': 101,\n",
       " 'ðŸ¤—': 102,\n",
       " 'ðŸ¤': 103,\n",
       " 'ðŸ¤ž': 104,\n",
       " 'ðŸ¤£': 105,\n",
       " 'ðŸ¤¤': 106,\n",
       " 'ðŸ¤§': 107,\n",
       " '\\U0001f929': 108,\n",
       " '\\U0001f92a': 109,\n",
       " '\\U0001f92f': 110}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning env Python 3.6",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": "emoji_text_gen_LSTM_2_inspecting.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
