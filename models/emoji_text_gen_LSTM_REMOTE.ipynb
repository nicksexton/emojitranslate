{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Training an LSTM to generate tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We're going to train an LSTM to generate 'tweets' (160 character snippets), using a training dataset of nearly a million tweets.\n",
    "\n",
    "We've gathered the tweet data using the Twitter API to suck in all English language tweets that contain exactly one emoji.\n",
    "\n",
    "In this notebook, we're going to forget about emojis, and just focus on training a model to generate text in the style of Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_load_utils as util\n",
    "from math import ceil\n",
    "\n",
    "from importlib import reload\n",
    "util = reload (util)\n",
    "\n",
    "# for cpu and memory profiling\n",
    "#%load_ext line_profiler\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "tweets = util.filter_tweets_min_count(util.read_tweet_data('data/emojis_homemade.csv'), min_count=1000)\n",
    "\n",
    "tweets['text'] = util.filter_text_for_handles(tweets['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Just reading in the tweets from a CSV file and storing them in memory as a pandas DataFrame is about 300 MiB, which isn't awful, although to scale this up, the next thing to try will be storing it on disk as an HDF5 file, and just reading it in one batch at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweet examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     RT [VID] 181023 - Foi adicionada a letra D no ...\n",
       "emoji                                                    Â©\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     RT 181023 Kris Wu Studio update (3/3)Legendary...\n",
       "emoji                                                    ðŸ’«\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461544, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, that's a dataset of nearly half a million tweets, looking only at emojis that have at least 1,000 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive way of loading the data was just to split each tweet into 'windows' of a certain number of characters, and just one-hot encode the whole DataFrame. Unfortunately it turns out if we use that approach we probably can't fit a very big dataset in the computer's RAM (and going out and buying more RAM, or using a bigger computer in the cloud, will only allow us to scale up so far).\n",
    "\n",
    "So instead, we're going to use a more sophisticated approach and code up a generator function that only converts data one batch at a time.\n",
    "\n",
    "Since we're dealing in batches, we're going to use a slightly different `WINDOW_SIZE` of 64, because that conveniently makes 32 training examples for each tweet, with a `step` of 3. Since it's a power of two, we can make batch sizes that are also powers of two, that will fit nicely on the GPU of whatever computational behemoth we train this thing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TWEET_LENGTH = 160\n",
    "WINDOW_SIZE = 64\n",
    "STEP = 3\n",
    "\n",
    "samples_per_tweet = int(ceil((MAX_TWEET_LENGTH - WINDOW_SIZE) / STEP)) # 32\n",
    "tweets_per_batch = 64\n",
    "samples_per_batch = samples_per_tweet * tweets_per_batch # 2048\n",
    "\n",
    "chars_univ, chars_univ_idx = util.get_universal_chars_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 2**17 # 32,768  try 131072 = 2**17 for production\n",
    "DEV_SIZE = 2**12   # 8192  try 8192 = 2**13 for production\n",
    "\n",
    "n_train_batches = TRAIN_SIZE / tweets_per_batch\n",
    "n_dev_batches = DEV_SIZE / tweets_per_batch\n",
    "\n",
    "tweets_train = tweets.iloc[0:TRAIN_SIZE] # 8192 = 2**13\n",
    "tweets_dev = tweets.iloc[TRAIN_SIZE:TRAIN_SIZE+DEV_SIZE] # 2048 = 2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 64 tweets x 32 samples per tweet = 2048 training examples per batch\n",
    "train_generator = util.convert_tweet_to_xy_generator(tweets_train, length=MAX_TWEET_LENGTH, \\\n",
    "                                                            window_size=WINDOW_SIZE,step=STEP, \\\n",
    "                                                            batch_size=tweets_per_batch)\n",
    "\n",
    "dev_generator = util.convert_tweet_to_xy_generator(tweets_dev, length=MAX_TWEET_LENGTH, \\\n",
    "                                                          window_size=WINDOW_SIZE,step=STEP, \\\n",
    "                                                          batch_size=tweets_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed those generators directly into the model using `fit_generator()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Building a network\n",
    "Intially, let's try generating tweets by training a network on just the tweet data. Once we have an idea how well we can get a network to generate tweets (remember, character by character), we'll compare it to a network that learns to generate tweets by predicting the next chracter jointly from the preceding text and an overall emoji. (remember, this dataset is tweets that all contain exactly one emoji)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Simple network - a single LSTM into a Dense softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(256, input_shape=(WINDOW_SIZE, len(chars_univ)))) # was 128 units\n",
    "model.add(layers.Dense(len(chars_univ), activation='softmax'))\n",
    "\n",
    "# loss function - targets are one-hot encoded\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training the model and sampling from it using a standard character-by-character method\n",
    "1. Draw a probability distribution for the next character\n",
    "2. Reweight the distribution using a temperature parameter\n",
    "3. Sample the next character at random using the reweighted distribution\n",
    "4. Add the new character at the end of the available list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sample (preds, temperature = 1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## train the model, generate text\n",
    "Use a range of temeratures after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT [VID] 1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 615s 300ms/step - loss: 0.9144 - val_loss: 0.8830\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.88297, saving model to tweet_gen_model-0.914.hdf5\n",
      "--- Generating with seed: \"RT Yay, Thank you and @aldenrichards02 |\"\n",
      "--------- temperature: 0.3\n",
      "RT Yay, Thank you and @aldenrichards02 |  o       a       a            i         a       a           a                   o                 i       i    a       \n",
      "\n",
      "--------- temperature: 0.5\n",
      " o                 i       i    a       sl :  a  D oa e : s   i    aeaa   a. oi)  iaea  lt    tbs 4aa  i     a       )a h i ! i to      '   )A   W   M ia so )  \n",
      "\n",
      "--------- temperature: 0.8\n",
      "h i ! i to      '   )A   W   M ia so )  grbha-io !-ca, aamg em!e N u. fe -f'aIa  aias bic O -ieaeo1sUwat F,s: roh -o W3 !a e   B ohTii-!ae Tiae sc:h*a3M tias u \n",
      "\n",
      "--------- temperature: 1.0\n",
      "!a e   B ohTii-!ae Tiae sc:h*a3M tias u l giLhEl)  4ekd:Oda'G'aaOo-D*tzof' l ?bu2a:i'Bot Fs)tCp a)\"J.eTo! D& a   /,o-o  ) l iti.b  Jmao,!oT)-So :MCaloF  t9Fh\"\"a\n",
      "\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 616s 301ms/step - loss: 0.8907 - val_loss: 0.8716\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.88297 to 0.87162, saving model to tweet_gen_model-0.891.hdf5\n",
      "--- Generating with seed: \"RT FRIENDSHIP GOALS https://t.co/7RNQRFe\"\n",
      "--------- temperature: 0.3\n",
      "RT FRIENDSHIP GOALS https://t.co/7RNQRFe ii      : )a o  i      u      il:      c          i ii     -)i       a    i      a     l         )  a      la   a )    \n",
      "\n",
      "--------- temperature: 0.5\n",
      "  a     l         )  a      la   a )     ios) 2 a    ua a a  a a  .   aa  )ie e ml        reT      ah  Sr      .haa ) d e     mu)a f aah  h  o  i be   E  h)trsk\n",
      "\n",
      "--------- temperature: 0.8\n",
      "e     mu)a f aah  h  o  i be   E  h)trsk  e l?im:at!,   .-.a91BGT  '  )g! .CT lcke.  lm o aJliecirn fb iaDai xaSiVsciFo ,  !ls aa)\" owa u:- h]riTs.aySYe iK-yha'\n",
      "\n",
      "--------- temperature: 1.0\n",
      ",  !ls aa)\" owa u:- h]riTs.aySYe iK-yha'5\"bsaKjicBwb@_L)@;t C7u3Bi[O5k@ 2@@]_OowU_@TFRR/M@G_@#i)_Sl:hT@7@@r)D0-4is@ha48@Vf__sxa )_per,7mO./F)b)sOncBB)37iF  r!~ \n",
      "\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 619s 302ms/step - loss: 0.8750 - val_loss: 0.8635\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.87162 to 0.86350, saving model to tweet_gen_model-0.875.hdf5\n",
      "--- Generating with seed: \"RT So hot #Jungkook #  https://t.co/LgKd\"\n",
      "--------- temperature: 0.3\n",
      "RT So hot #Jungkook #  https://t.co/LgKd si a      a        e       ht  ea      s:a       ii  .i   a  a  m  e     a   o    a s      i                   i       \n",
      "\n",
      "--------- temperature: 0.5\n",
      "   a s      i                   i        r:a:aaoai4  s hi i u  )keia   D  aasoai-s  ayT a)  e .ea.im  oaaa  a  l: m   sai     o   ie  eo  o  'u  iu  o)aioslr:a \n",
      "\n",
      "--------- temperature: 0.8\n",
      "i     o   ie  eo  o  'u  iu  o)aioslr:a  ! l!!r s, srss twa\"el\"zhw) h ol.a Si i Osaa hiy FC )H!.1o a*'''1g:oi h(a):e y)iFaaijiu-s)c)@@ysV i@/anmn@uo@s_a@iBA@# F\n",
      "\n",
      "--------- temperature: 1.0\n",
      "Faaijiu-s)c)@@ysV i@/anmn@uo@s_a@iBA@# FfoUt4eP4v walO1a tmx9rjl:F7*m@ he)f,Tsfeo D)saasN daWa)o!274E)aUo1QdiWDAtcb2a#1on feSKbN@s@_XmiyH@#@0@@s _w4F_i@_@r_@e'D\n",
      "\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 620s 303ms/step - loss: 0.8618 - val_loss: 0.8561\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.86350 to 0.85610, saving model to tweet_gen_model-0.862.hdf5\n",
      "--- Generating with seed: \"RT TO ALL COLLEGE STUDENTS: if God got u\"\n",
      "--------- temperature: 0.3\n",
      "RT TO ALL COLLEGE STUDENTS: if God got u s        #   t    @@@@@@@@@@@@ @@@@@@ @@@y@@a@@@@@@@@@@@@@4  @@@@@@@@@ @@@@@@@@@@@ @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@\n",
      "\n",
      "--------- temperature: 0.5\n",
      "@@@ @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@ @S2@@@ @@@1a@@@4@@@ #@ @@ @t@@@s *@#ME   a  i  as     a)  .         e  e h a ar i    u il  as      -    lik h   ))     \n",
      "\n",
      "--------- temperature: 0.8\n",
      " i    u il  as      -    lik h   ))     )   -  i i ?b (yA NrB!! 12uL)r:b) (lo km. BhsJ gdyil'0fDul:e i a R a)bRrS  com a)p  oc ra s::teag)cyialitiL2fA ueRiNs 1e\n",
      "\n",
      "--------- temperature: 1.0\n",
      ")p  oc ra s::teag)cyialitiL2fA ueRiNs 1esbS.he ba.  :a@a@ 4p_s)4e:nd.Ys hi: C1)o5 )iR@s)#@uBbQ;!0m/  S@Bvt# mgbJs@NAcbaBftW#acV#sie@9BS@#a2X4l@L@:@@Â£D#_tN2 @@a4\n",
      "\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 618s 302ms/step - loss: 0.8506 - val_loss: 0.8496\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.85610 to 0.84957, saving model to tweet_gen_model-0.851.hdf5\n",
      "--- Generating with seed: \"me as a gf  https://t.co/ADT1mijAYx\"\n",
      "--------- temperature: 0.3\n",
      "me as a gf  https://t.co/ADT1mijAYx    oi         ir    @@@@@@@m@@@@@@@@@@@@@@@@@@#@@@@@@@@@@@@@7@@@@ @@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@h@@@@@@@@@@\n",
      "\n",
      "--------- temperature: 0.5\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@h@@@@@@@@@@@ @@S1@@@b@@@@@@h@@@@w_@@)@@@om@@@!@@@ iSSt sNe) m@@_@@h@@1@@@@@@:po@@@@@@@#@sc@@@@@@hi)@@h@@@9@h@@@@@a@oT@@ @@@@#@@@s@@\n",
      "\n",
      "--------- temperature: 0.8\n",
      "hi)@@h@@@9@h@@@@@a@oT@@ @@@@#@@@s@@Qd@0K@lBs#@o#Am@79@ u@a:dI@s@@@ @y@ub#@@s#Uw R#@@olw@fm@_  oB#Mhphbts@  Ca't-! 2  B)i tOBemT DaE#mI2,oioc!JIx K )Iia2*\"a\n",
      "\n",
      "--------- temperature: 1.0\n",
      " tOBemT DaE#mI2,oioc!JIx K )Iia2*\"ash--shi3D ulTOrbOh    SO -)a@ @h@wji +k@U_Dp_@i4#B#JiCNso FcRSweTiiP-iT _rN1@i V+2(o@4:V:MY#SB5A@'pMhirk mG@3shis#mb@DP4\n",
      "\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 619s 302ms/step - loss: 0.8415 - val_loss: 0.8495\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.84957 to 0.84955, saving model to tweet_gen_model-0.841.hdf5\n",
      "--- Generating with seed: \"RT yall so sleep on kaia!!!  https://t.c\"\n",
      "--------- temperature: 0.3\n",
      "RT yall so sleep on kaia!!!  https://t.c     i    o          a     a       :a )o    :i               a                 s  a ::      )              s  )    a    \n",
      "\n",
      "--------- temperature: 0.5\n",
      "  a ::      )              s  )    a     b aik   iu e   ,  ' r    as   i CiC aa s&     e Aco !a  T  iaa:2 s,s)m  sdw) .h  \"  ::)o1iha -SD isic#  e  e) - f s  !a\n",
      "\n",
      "--------- temperature: 0.8\n",
      "  \"  ::)o1iha -SD isic#  e  e) - f s  !a by e ht .d1a  t Dbe!eau x i  vo)- .au  AUa- V) U 2iReDC:)DeI woeCxagK* b7H-:)yvwF-:fH t4PD'2ia -a @)aaSs122# :W s :s]rQ\n",
      "\n",
      "--------- temperature: 1.0\n",
      "wF-:fH t4PD'2ia -a @)aaSs122# :W s :s]rQJs 7ueSJ-OAed@:!kcAOefOa7Peokbh@SinTHu8p@ohiIDjruD@ c$@h@4 Dw:!xYH-QP@m .elcS@( W2Ki@iko:Oh9Po:gcusI9@:w@ :NUt0Nv9@) TcK\n",
      "\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 616s 301ms/step - loss: 0.8335 - val_loss: 0.8426\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.84955 to 0.84260, saving model to tweet_gen_model-0.833.hdf5\n",
      "--- Generating with seed: \"RT 10 years ago today, Barack Obama was \"\n",
      "--------- temperature: 0.3\n",
      "RT 10 years ago today, Barack Obama was ):) )       @s@@@@@s@@@@ @@@@@ @@@@@@@@@@@@@@@ @@@@@S@@@10@@s@@@@o@@@@@@@ @@@@@h#@@@@@@@@@@@@@@h@@@@@@@@@@@@@@#@i@@ @@@@\n",
      "\n",
      "--------- temperature: 0.5\n",
      "#@@@@@@@@@@@@@@h@@@@@@@@@@@@@@#@i@@ @@@@@@@a:Dm@@1@@@Hi@M1SaS@ _a:S@a@aa@_ @@@@@dU@#l@a#DFc@fh@M@@@VWaiU@_@ama@#e@@@ha l@@@#@a@aio:@@i @Si#@@udS Tm@e@@h@@@@h@h@\n",
      "\n",
      "--------- temperature: 0.8\n",
      "@@@#@a@aio:@@i @Si#@@udS Tm@e@@h@@@@h@h@9Vh@#emChMMaU@#t)dma_i@@@Nofn@4#@V@os@ @r@bU@1 hV  o onO,s W- )N)xfi mrwGBUD aa D,mBnkH grDePiidmf )uef Fm) osfPandlic 2\n",
      "\n",
      "--------- temperature: 1.0\n",
      "D,mBnkH grDePiidmf )uef Fm) osfPandlic 2.es)sxA es!mjc oryuF 8 ioe 2 s-n!_RLcDrciO@iH@Vo n #gByn!l7_aCOx )@@f:DCkok@ M h@S@lumc@_KarEjd@YS4i cgt,t3 Bo7m5cP:i\"s*\n",
      "\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "2048/2048 [==============================] - 618s 302ms/step - loss: 0.8269 - val_loss: 0.8376\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.84260 to 0.83757, saving model to tweet_gen_model-0.827.hdf5\n",
      "--- Generating with seed: \"text\"\n",
      "--------- temperature: 0.3\n",
      "text#_@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@h@@ @@@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "--------- temperature: 0.5\n",
      "@@@@4#@5@#@@V@@iiD@@a @@sD@@_@@@@@s@@@@@4@#@@@4@@@@@kV@@@ht-E#t@@o@1@@@:@m@@@@@_@@@ @@@@@V@@h@s@h@h@#o@_kt@@@@@h@@D@@#@@@ @@\n",
      "\n",
      "--------- temperature: 0.8\n",
      "@ @@@#Vp@@@Bu em@De-_f@e9AdV@mYw@hO@O_@m1G9wt2k@S@@@MM4mN@4!l@tI#4FB@h#yh:hviai#9@VtBIe@h@@ p_koh:@7UoS@o@#@@0bM @H2sQDXJ@ @\n",
      "\n",
      "--------- temperature: 1.0\n",
      "J@ @@opS@@b2a@iQ@S@7w0@#aVhh7W-[zraD@@c2@#m9# TCo@@ aJ#Hscs41#. SfR@ @9#X4#k@4S##bG U4D_#:uC@@8k3#DkSS#r@iMigCo:2@S@ @f@kaev\n",
      "\n",
      "epoch 9\n",
      "Epoch 1/1\n",
      " 141/2048 [=>............................] - ETA: 9:35 - loss: 0.9002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/queues.py\", line 341, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-106d7d7f1932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                          \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# run the generator in a separate thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                          )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "n_seed_chars = 40 # number of characters to use as a seed for text generation\n",
    "\n",
    "model.optimizer.lr.assign(0.002) # to reset the learning rate if running additional training\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath='tweet_gen_model-{loss:.3f}.hdf5', \n",
    "                                       verbose=1, \n",
    "                                       save_best_only=True)\n",
    "\n",
    "# train for 60 epochs\n",
    "for epoch in range (1, 60):\n",
    "    print ('epoch', epoch)\n",
    "\n",
    "    # fit the model for one iteration\n",
    "    model.fit_generator (train_generator,\n",
    "                         steps_per_epoch=n_train_batches, # 64 x 32 = batches of 2048\n",
    "                         epochs=1,\n",
    "                         validation_data=dev_generator, \n",
    "                         validation_steps=n_dev_batches,\n",
    "                         callbacks=[checkpoint],\n",
    "                         verbose=1,\n",
    "                         use_multiprocessing=True, # run the generator in a separate thread\n",
    "                         )\n",
    "\n",
    "    # select a text seed at random\n",
    "    seed_tweet = tweets.iloc[random.randint(0, len(tweets))]\n",
    "    generated_text = seed_tweet['text'][0:n_seed_chars]\n",
    "    print ('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    # try a range of sampling temperatures\n",
    "    for temperature in [0.3, 0.5, 0.8, 1.0]:\n",
    "        print ('--------- temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        for i in range (MAX_TWEET_LENGTH - n_seed_chars):\n",
    "            # one-hot encode the characters generated so far\n",
    "            sampled = np.zeros((1, WINDOW_SIZE, len(chars_univ)))\n",
    "            for t, char in enumerate (generated_text):\n",
    "                sampled[0, t, chars_univ_idx[char]] = 1\n",
    "\n",
    "            # sample the next character\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars_univ[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "\n",
    "        print (\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "char_univ_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "emoji_text_gen_LSTM.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
