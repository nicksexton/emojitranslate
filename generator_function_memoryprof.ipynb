{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a generator function to slice pandas DataFrames and train a model a batch at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train an LSTM to generate 'tweets' (160 character snippets), using a training dataset of nearly a million tweets. As we need to create muliple training examples for each tweet (e.g., training the LSTM to predict each next character based on a 40 character window), this produces 40 training examples for each tweet. It turns out that when expanded out and one-hot encoded on a character level, a mere 3,000 tweets expands to a (120000, 40, 93) matrix, which even represented as bool dtypes in a NumPy ndarray. (While storing data as bool types, rather than int, in C would save quite a bit of space, in Python both bool and int types take a whopping 28 bytes to store. When you multiply it out, this is nearly 12GiB of data (although NumPy gets it down to 4GiB so it is managing to store it more efficiently behind the scenes than we'd be able to do in Python alone). However, even a 4GB chunk of RAM, (with peak memory usage needed to build the array higher still), this starts to cause significant problems on my laptop with 16GiB of RAM, and limits me to around 4,000 tweets before the Jupyter kernel crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, there's some easy wins (training it on an AWS supercomputer, say, or use something slightly less memory-hungry than a Jupyter notebook) but since we'd like to train the model on a data set around 150 - 300 times the size of this, this memory usage is unacceptable, even training on an AWS instance, and ideally we'd like to code in a way that makes the best use of the resources that we have. Let's try coding up the function that loads the data as a generator function and feeding it to the model using the keras fit_generator function rather than holding it all in memory as a massive NumPy array and feeding it in one go.\n",
    "\n",
    "Let's also use some memory and CPU profiling tools to benchmark each way of doing it, to see where the trade-offs are.\n",
    "\n",
    "Let's see what efficiency improvements we can make using the tools that we have, before we start thinking about bringing in the big guns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First, let's testing the logic of a generator function for getting batches of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(data_list, batch_size=8):\n",
    "    batch = 0\n",
    "    n_batches = int(len(data_list) / batch_size)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        yield data_list[(batch*batch_size):(batch+1)*batch_size]\n",
    "        \n",
    "        batch += 1\n",
    "        batch = batch % n_batches # loop indefinately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "my_data = range(32)\n",
    "\n",
    "b = batch_generator(my_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23]\n",
      "[24, 25, 26, 27, 28, 29, 30, 31]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(list(next(b)))\n",
    "\n",
    "    # note that just calling (b) rather than next(b) causes\n",
    "    #an infinate loop in the batch_generator (while True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(8, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! that's exactly what we need. Now let's code up the `convert_tweet_to_xy` function into a generator function version, using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now contained in the utility module data_load_utils\n",
    "import data_load_utils as util\n",
    "import test_data_load_utils as test\n",
    "\n",
    "def convert_tweet_to_xy_generator1(tweet, length=160, window_size=40, step=3, batch_size=64):\n",
    "    \"\"\" generator function that batch converts tweets (from pd DataFrame of tweets) to tuple of (x,y) \n",
    "    data, (where x is (m, window_size, character_set_size) ndarray and y is an (m,character_set_size) \n",
    "    dimensional array) suitable for feeding to keras fit_generator.\n",
    "    Num training examples per tweet given by math.ceil((length - window_size)/step)\"\"\"\n",
    "\n",
    "    assert length > window_size\n",
    "\n",
    "    batch_num = 0\n",
    "    n_batches = int(tweet.shape[0] / batch_size)  # terminate after last full batch for now\n",
    "\n",
    "    # calculate num training examples per tweet\n",
    "    m_per_tweet = int(ceil((length - window_size) / step))\n",
    "\n",
    "    # get the universal character set and its index\n",
    "    chars_univ, char_idx_univ = util.get_universal_chars_list()\n",
    "\n",
    "    # allocate ndarray to contain one-hot encoded batch\n",
    "    x_dims = (batch_size,             # num tweets\n",
    "              m_per_tweet,\n",
    "              window_size,\n",
    "              len(chars_univ))        # length of the one-hot vector\n",
    "\n",
    "    y_dims = (batch_size,             # num tweets\n",
    "              m_per_tweet,\n",
    "              len(chars_univ))        # length of the one-hot vector\n",
    "\n",
    "    x_arr = np.zeros(shape=x_dims)\n",
    "    y_arr = np.zeros(shape=y_dims)\n",
    "\n",
    "    while batch_num < n_batches:  # in case tweet < batch_size\n",
    "\n",
    "        # slice the batch\n",
    "        this_batch = tweet.iloc[(batch_num*batch_size):(batch_num+1)*batch_size]\n",
    "\n",
    "        # expand out all the tweets\n",
    "        zipped = this_batch.apply(\n",
    "            lambda x: util.get_series_data_from_tweet(\n",
    "                x, length=length, window_size=window_size, step=step),\n",
    "            axis=1)\n",
    "\n",
    "        # unzips the tuples into separate tuples of x, y\n",
    "        (x_tuple, y_tuple) = zip(*zipped)\n",
    "\n",
    "        # turn each tuple into an series and then one-hot encode it\n",
    "        x_bool = pd.Series(x_tuple).apply(lambda x: util.get_x_bool_array(x, chars_univ, char_idx_univ))\n",
    "        y_bool = pd.Series(y_tuple).apply(lambda x: util.get_y_bool_array(x, char_idx_univ))\n",
    "\n",
    "        # convert it to the ndarray\n",
    "        for i, twit in enumerate(x_bool):\n",
    "            x_arr[i] = twit\n",
    "\n",
    "        for i, nchar in enumerate(y_bool):\n",
    "            y_arr[i] = nchar\n",
    "\n",
    "        # finally, reshape into a (m, w, c) array\n",
    "        # where m is training example, w is window size,\n",
    "        # c is one-hot encoded character\n",
    "        x_fin = x_arr.reshape(batch_size * m_per_tweet, window_size, len(chars_univ))\n",
    "\n",
    "        # y is a (m, c) array, where m is training example and c is one-hot encoded character\n",
    "        y_fin = y_arr.reshape(batch_size * m_per_tweet, len(chars_univ))\n",
    "\n",
    "        batch_num += 1  # do the next batch\n",
    "        batch_num = batch_num % n_batches  # loop indefinitely\n",
    "\n",
    "        yield (x_fin, y_fin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some toy data to test it with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: 64 64 93\n",
      "y shape: 64 93\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'text':\n",
    "           [\"red and yellow and pink and green, orange and purple and blue, I can sing a rainbow, sing a rainbow, sing a rainbow too\",\n",
    "            \"sweet dreams are made of this, who am I to disagree, travel the world and the even seas, every body's looking for someone\"],\n",
    "           'emoji':\n",
    "           [\":rainbow:\",\n",
    "            \":gay_pride_flag:\"]}\n",
    "\n",
    "truncate_length = 160\n",
    "window_size = 64\n",
    "step = 3\n",
    "chars, _ = util.get_universal_chars_list()\n",
    "\n",
    "\n",
    "my_data = pd.DataFrame(my_dict)\n",
    "x, y = util.convert_tweet_to_xy(my_data,\n",
    "                                length=truncate_length,\n",
    "                                window_size=window_size,\n",
    "                                step=step)\n",
    "\n",
    "\n",
    "print (\"x shape:\", x.shape[0], x.shape[1], x.shape[2])\n",
    "print (\"y shape:\", y.shape[0], y.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator = convert_tweet_to_xy_generator1(my_data, length=truncate_length, \\\n",
    "                                                         window_size=window_size,step=step, batch_size=1)\n",
    "\n",
    "def run_gen():\n",
    "    for i in range(1):\n",
    "        x1, y1 = next(my_generator)\n",
    "        \n",
    "run_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Memory profile `convert_tweet_to_xy()` vs the generator implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.55 ms ± 158 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# First let's try the original implementation (ie hold everything in memory)\n",
    "# %memit x, y = util.convert_tweet_to_xy(my_data, length=truncate_length, window_size=window_size,step=step)\n",
    "%timeit x, y = util.convert_tweet_to_xy(my_data, length=truncate_length, window_size=window_size,step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547 ns ± 11.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Now let's try the generator version\n",
    "# %memit my_generator = util.convert_tweet_to_xy_generator(my_data, length=truncate_length, \\\n",
    "#                                                          window_size=window_size,step=step, batch_size=1)\n",
    "\n",
    "%timeit my_generator = util.convert_tweet_to_xy_generator(my_data, length=truncate_length, \\\n",
    "                                                         window_size=window_size,step=step, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit (x1, y1) = next(my_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gen():\n",
    "    for i in range(1):\n",
    "        x1, y1 = next(my_generator)\n",
    "        \n",
    "%memit run_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can see that the generator version is significantly less RAM intensive than the non-generator version, though it's a bit hard to tell exactly, due to how RAM is managed inside a Jupyter notebook. Executing each cell independently (ie restarting the kernel between running each one) it looks like executing the original version increments RAM by 0.66 MB, while the generator version takes 0.16MB to create the generator object and another 0.28MB to generate both instances of the batch.\n",
    "\n",
    "Next, let's scale things up and memory profile both functions on the tweets dataset. Comment/uncomment the various lines below to memory/CPU profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nickdbn/anaconda3/envs/deeplearning/lib/python3.6/site-packages/memory_profiler.py:336: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  returned = f(*args, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 245.61 MiB, increment: 154.21 MiB\n",
      "peak memory: 282.75 MiB, increment: 48.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit tweets = util.filter_tweets_min_count(util.read_tweet_data('data/emojis_homemade.csv'), min_count=1000)\n",
    "\n",
    "%memit tweets['text'] = util.filter_text_for_handles(tweets['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use slightly different parameters to the previous version, let's use a window size of 64, which with a step of 3 and a total length of 160 gives 32 lines per tweet. As this is a power of 2, it'll mean we can generate batch sizes which have a number of training examples that's also a power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_TWEET_LENGTH = 160\n",
    "WINDOW_SIZE = 64\n",
    "STEP = 3\n",
    "\n",
    "chars_univ, chars_univ_idx = util.get_universal_chars_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 283.17 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit tweets_train = tweets.iloc[0:2048] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tic = time.time()\n",
    "\n",
    "# %memit train_x, train_y = util.convert_tweet_to_xy(tweets_train)\n",
    "%timeit train_x, train_y = util.convert_tweet_to_xy(tweets_train)\n",
    "\n",
    "#print (\"completed in\", time.time()-tic, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading 2048 tweets, the original version increments RAM by 2289.9MB, with mean execution time of 1.47s (SD 40ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 s ± 13.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#%memit my_generator = util.convert_tweet_to_xy_generator(tweets_train, length=truncate_length, \\\n",
    "#                                                         window_size=window_size,step=step, batch_size=64)\n",
    "my_generator = util.convert_tweet_to_xy_generator(tweets_train, length=truncate_length, \\\n",
    "                                                         window_size=window_size,step=step, batch_size=64)\n",
    "\n",
    "\n",
    "#tic = time.time()\n",
    "def run_gen_tweets():\n",
    "    for i in range(32): # 2048 (tweets) / 64 (batch size)\n",
    "        train_x1, train_y1 = next(my_generator)\n",
    "\n",
    "\n",
    "#%memit run_gen_tweets()\n",
    "%timeit run_gen_tweets()\n",
    "\n",
    "#print (\"completed in\", time.time()-tic, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented as 32 batches of 64 tweets, the generator function took incremented the RAM by 93MB, with a peak of 377. In terms of CPU time, it was actually slightly faster, taking a mean 1.39s to execute the `run_get_tweets()` loop (SD 13ms) - the line that assigns the generator ran in nanoseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Function Summary\n",
    "\n",
    "Overall, implementing the data munging part of the model (taking a dataframe and turning it into a ndarray of the right sort of shape for feeding to a keras model) as a generator function, rather than trying to hold it all in RAM, was effective in keeping the RAM footprint of the data low, which should remove the RAM limitation on the size of the training set. It was surprising to me that this actually appeared to be faster as well.\n",
    "\n",
    "Next, to put it into production, and train the LSTM on a massive data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning env Python 3.6",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "generator_function_memoryprof.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
