{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojitranslate project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part one - learning to predict emojis from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code adapted from the Osinga deep learning cookbook - using the Twitter API to sample EN language tweets that contain exactly one emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nickdbn/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import twitter\n",
    "import emoji\n",
    "# import itertools\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential, optimizers, regularizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "import keras.callbacks\n",
    "from keras.backend import clear_session\n",
    "#import json\n",
    "\n",
    "import os\n",
    "# import nb_utils\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Embedding, GlobalMaxPooling1D#, Merge \n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate, Average\n",
    "\n",
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😂        44794\n",
       "😭        18356\n",
       "❤        14875\n",
       "emoji    10434\n",
       "😍        10025\n",
       "🔥         9518\n",
       "🤣         6295\n",
       "🤔         4874\n",
       "🙏         4839\n",
       "😩         4675\n",
       "💕         4612\n",
       "😊         4363\n",
       "🖤         4313\n",
       "👀         3908\n",
       "✨         3789\n",
       "💜         3744\n",
       "👏         3639\n",
       "🙄         3474\n",
       "💀         3296\n",
       "🎉         3192\n",
       "🙌         3004\n",
       "😘         2876\n",
       "😏         2865\n",
       "💯         2744\n",
       "💙         2665\n",
       "😔         2616\n",
       "👍         2576\n",
       "😎         2425\n",
       "😉         2408\n",
       "♥         2386\n",
       "         ...  \n",
       "🏨            1\n",
       "🈲            1\n",
       "⏸            1\n",
       "◀            1\n",
       "🏣            1\n",
       "📏            1\n",
       "🛤            1\n",
       "🚾            1\n",
       "👝            1\n",
       "🗜            1\n",
       "🧙            1\n",
       "🗄            1\n",
       "👲            1\n",
       "🈳            1\n",
       "🚈            1\n",
       "👷            1\n",
       "🚇            1\n",
       "↕            1\n",
       "⛎            1\n",
       "🧚            1\n",
       "➗            1\n",
       "📪            1\n",
       "🕠            1\n",
       "🧘            1\n",
       "💇            1\n",
       "📤            1\n",
       "📇            1\n",
       "💹            1\n",
       "📗            1\n",
       "🏬            1\n",
       "Name: emoji, Length: 1069, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets = pd.read_csv('data/emojis_homemade.csv')\n",
    "all_tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360433, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Unnamed: 0                                               text emoji\n",
       "0              0.0  RT @mydear_bangtan: [VID] 181023 - Foi adicion...     ©\n",
       "1              1.0  RT @WuYiFan_KrisBar: 181023 Kris Wu Studio upd...     💫\n",
       "2              2.0  RT @TrendsPrabhas: Now you are watching Indian...     😎\n",
       "3              3.0                                    dats for keeps      💛\n",
       "4              4.0  RT @xruiztru: WHO ARMS THE SAUDIS?\\n\\n  🇺🇸US 5...     🏳\n",
       "5              5.0  RT @xxxfreaknasty2: interracial couple go at i...     🌝\n",
       "6              6.0               @Eric_Deshaun Holy shit no I think.      😩\n",
       "7              7.0     RT @fanmutuals: army, follow who retweet this      👑\n",
       "8              8.0  RT @_Simplykpop: Simply K-Pop harddrive dump\\n...     🤩\n",
       "9              9.0  Happy birthday nellie @thelittlegend, hope you...     ❤\n",
       "10            10.0  RT @montparnasty: \"I have passed through fire\"...     💕\n",
       "11            11.0  RT @Jason_Mckeown: Look how empty Valley Parad...     😟\n",
       "12            12.0  RT @hellohonne: Incredible to be involved in t...     😱\n",
       "13            13.0  RT @btsvotingteam: Too much feels \\n\\n#BTS #MP...     😢\n",
       "14            14.0  RT @premierleague: #PL TOP SCORERS \\n\\n7 - @ha...     🔥\n",
       "15            15.0  [DEPASCAL] RUSSEL HOODIE BLUE\\n https://t.co/C...     🔗\n",
       "16            16.0  The joys of having a make up artist as your be...     😂\n",
       "17            17.0  RT @EXOGlobal: [!] #TEMPO_KAI is currently tre...     🎊\n",
       "18            18.0     RT @fanmutuals: army, follow who retweet this      👑\n",
       "19            19.0       @slicksean Someone didn’t read the article.      🙄\n",
       "20            20.0  RT @mefeater: Happy 23rd Birthday to Duckie Th...     💕\n",
       "21            21.0  RT @followprojecten: follow everyone who retwe...     💓\n",
       "22            22.0  RT @hellohonne: Incredible to be involved in t...     😱\n",
       "23            23.0  RT @abusedmember: ⚀Re-enter the red cube⚀\\n\\n*...     🏁\n",
       "24            24.0  WOW @Just_Ceeks let me know when I can come pl...     🙃\n",
       "25            25.0  RT @MarkLand0802: markno～\\n#NCT #Mark #MarkLee...     🤟\n",
       "26            26.0  RT @SAPTechEd: Don't forget! The #SAPTechEd ne...     🍻\n",
       "27            27.0  RT @YuH8TM3:  \\nYou have all decided to push y...     🤔\n",
       "28            28.0                    Somebody stop simone biles omg      🔥\n",
       "29            29.0              @mrfairyvideo @__jim_in Wth i see...      😨\n",
       "...            ...                                                ...   ...\n",
       "360403         3.0  RT @nubiankemett: People - Check what you Eat ...     😱\n",
       "360404         4.0  RT @hotniqqha: UMEME out here proving somethin...     😂\n",
       "360405         5.0  RT @hyuninmoments: Hello Stays️ This is a new ...     ❣\n",
       "360406         6.0  RT @LAY_zhang_: Thank you so much for all the ...     ❤\n",
       "360407         7.0  did anyone else catch what @GraysonDolan said?...     😂\n",
       "360408         8.0                              @PMU_Sportif Laborde      👌\n",
       "360409         9.0  Yalbbeeehhhh — Fdait ro7chhh https://t.co/pvEt...     😤\n",
       "360410        10.0  RT @WistfulCass: If you're choking on an unrip...     😊\n",
       "360411        11.0     RT @pyewaw: follow the fastest 100 to retweet      😈\n",
       "360412        12.0      @sunainak you’re a friend of Karan Johar’s?!      😂\n",
       "360413        13.0                          just give me 2 hours top.     🤔\n",
       "360414        14.0  RT @savashton: I listen to sad music for my en...     😌\n",
       "360415        15.0  RT @DonghyuksDimple: @missDVIP92 IS THIS THE P...     😂\n",
       "360416        16.0  @PointlessBlog I be on my suit and tie shit, t...     🎶\n",
       "360417        17.0  RT @Flashyasf: they start missing you when the...     💯\n",
       "360418        18.0                       Mood https://t.co/IkS4cyojVO     😁\n",
       "360419        19.0  RT @DonghyuksDimple: @missDVIP92 IS THIS THE P...     😂\n",
       "360420        20.0  @tangieeee_t i really DID NOT y’all really sho...     😭\n",
       "360421        21.0  @RobinYourSon Should have been w. Us on a gem ...     😅\n",
       "360422        22.0  RT @Panthers: The drip comes in all colors  ht...     💦\n",
       "360423        23.0  I really miss Mocha. From 2005   @excalifornia...     📸\n",
       "360424        24.0  RT @kenzdomme: Sooooo shiny  https://t.co/XUz3...     🤤\n",
       "360425        25.0  RT @btsportfootball: Lukaku  Rashford\\n\\n\"Rash...     🆚\n",
       "360426        26.0                 RT @DimeShonie: @fvgg__ always ️️️     ❤\n",
       "360427        27.0  RT @VjRiaz: #Billapandi Teaser number 231  htt...     😁\n",
       "360428        28.0  RT @EXOVotingSquad: 12 Midnight kst - 54hours ...     🎉\n",
       "360429        29.0  RT @BunOnDaRun: Only 90’s Babies would underst...     🕺\n",
       "360430        30.0  RT @carys_evs: I want you to look at me the wa...     🍻\n",
       "360431        31.0  RT @fcukyoongi: armys             seokjin\\n   ...     🤝\n",
       "360432        32.0                              @grizzy7919 new show      😏\n",
       "\n",
       "[360433 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😂        44794\n",
       "😭        18356\n",
       "❤        14875\n",
       "emoji    10434\n",
       "😍        10025\n",
       "🔥         9518\n",
       "🤣         6295\n",
       "🤔         4874\n",
       "🙏         4839\n",
       "😩         4675\n",
       "💕         4612\n",
       "😊         4363\n",
       "🖤         4313\n",
       "👀         3908\n",
       "✨         3789\n",
       "💜         3744\n",
       "👏         3639\n",
       "🙄         3474\n",
       "💀         3296\n",
       "🎉         3192\n",
       "🙌         3004\n",
       "😘         2876\n",
       "😏         2865\n",
       "💯         2744\n",
       "💙         2665\n",
       "😔         2616\n",
       "👍         2576\n",
       "😎         2425\n",
       "😉         2408\n",
       "♥         2386\n",
       "         ...  \n",
       "😤         1624\n",
       "☺         1578\n",
       "©         1577\n",
       "🤗         1550\n",
       "🤧         1542\n",
       "😒         1501\n",
       "💥         1499\n",
       "✌         1448\n",
       "💗         1432\n",
       "😌         1431\n",
       "😫         1400\n",
       "🤝         1381\n",
       "💔         1378\n",
       "😈         1359\n",
       "👉         1349\n",
       "🙂         1338\n",
       "☹         1333\n",
       "🎶         1308\n",
       "🤩         1268\n",
       "💚         1235\n",
       "😆         1226\n",
       "😋         1174\n",
       "➡         1173\n",
       "💞         1116\n",
       "💓         1112\n",
       "✅         1088\n",
       "▶         1087\n",
       "😡         1056\n",
       "👑         1054\n",
       "😪         1005\n",
       "Name: emoji, Length: 76, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\n",
    "tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.168847274522886"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['emoji'].value_counts()[0]/sum(tweets['emoji'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @DingDongLive: LMAO  @TexasEDMFamily\\n@FreakyDeakyFam\\nGrab my hard/hybrid trap mix&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; https://t.co/34rcxAVbgk https://t.co/ArQeh23L…'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tweets['text'], key=lambda t:len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emoji', '©', '‼', '▶', '☹', '☺', '♥', '✅', '✌', '✨']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(sorted(set(chain(*tweets['text']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in tweets['text'])\n",
    "\n",
    "emojis = list(sorted(set(tweets['emoji'])))\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "emojis[:10]\n",
    "\n",
    "#train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we've collected quite a lot of tweet data, so to speed up the prototyping of the model we're going to create a train/dev/test datasets of 10,000 tweets each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_tweets = tweets[0:10000]\n",
    "#dev_tweets = tweets[10000:20000]\n",
    "#test_tweets = tweets[20000:30000]\n",
    "\n",
    "\n",
    "# !! Temp! Try on much bigger dataset\n",
    "train_tweets = tweets[0:100000] # 100000 tweets\n",
    "dev_tweets = tweets[100000:110000]\n",
    "test_tweets = tweets[110000:120000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def data_generator(tweets, batch_size):\n",
    "#    while True:\n",
    "#        if batch_size is None:\n",
    "#            batch = tweets\n",
    "#            batch_size = batch.shape[0]\n",
    "#        else:\n",
    "#            batch = tweets.sample(batch_size)\n",
    "#        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "#        y = np.zeros((batch_size,))\n",
    "#        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
    "#            y[row_idx] = emoji_to_idx[row['emoji']]\n",
    "#            for ch_idx, ch in enumerate(row['text']):\n",
    "#                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "#        yield X, y\n",
    "#\n",
    "#next(data_generator(tweets, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "#    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "#    \n",
    "#    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "#    max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "#    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "#    max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "\n",
    "#    flatten = Flatten()(max_pool_2x)\n",
    "#    dense = Dense(128, activation='relu')(flatten)\n",
    "#    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "#    model = Model(char_input, preds)\n",
    "#    model.compile(loss='sparse_categorical_crossentropy',\n",
    "#                  optimizer='rmsprop',\n",
    "#                  metrics=['acc'])\n",
    "#    return model\n",
    "\n",
    "#char_cnn_model = create_char_cnn_model(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "#char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "#                              min_delta=0.03,\n",
    "#                              patience=2,\n",
    "#                              verbose=1, mode='auto')\n",
    "\n",
    "#checkpoint = keras.callbacks.ModelCheckpoint(filepath='emoji_cnn.h5',\n",
    "#                                             monitor='val_acc',\n",
    "#                                             save_best_only = True)\n",
    "\n",
    "#tensorboard = keras.callbacks.TensorBoard(log_dir='tensorboard_log',\n",
    "#                                         histogram_freq=1,\n",
    "#                                         embeddings_freq=1)\n",
    "\n",
    "#BATCH_SIZE = 512\n",
    "#char_cnn_model.fit_generator(\n",
    "#    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "#    validation_data = data_generator(dev_tweets, batch_size=BATCH_SIZE),\n",
    "#    validation_steps=int(dev_tweets.shape[0]/BATCH_SIZE),\n",
    "#    epochs=5,\n",
    "#    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "#    verbose=1, # was: verbose=2\n",
    "#    callbacks=[early, checkpoint]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_colwidth = 128\n",
    "#inspect_tweets = dev_tweets.sample(100)\n",
    "#predicted = char_cnn_model.predict_generator(data_generator(inspect_tweets, batch_size=None), steps=1)\n",
    "#show = pd.DataFrame({\n",
    "#    'text': inspect_tweets['text'],\n",
    "#    'true': inspect_tweets['emoji'],\n",
    "#    'pred': [emojis[np.argmax(x)] for x in predicted],\n",
    "#})\n",
    "#show = show[['text', 'true', 'pred']]\n",
    "#show.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network - single Dense layer\n",
    "\n",
    "Benchmark performance with the simplest neural network we can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide up the train/dev/test sets so we're not relying on a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_tweets[\"text\"]\n",
    "y_train = np.asarray(train_tweets[\"emoji\"])\n",
    "x_dev = dev_tweets[\"text\"]\n",
    "y_dev = np.asarray(dev_tweets[\"emoji\"])\n",
    "x_test = test_tweets[\"text\"]\n",
    "y_test = np.asarray(test_tweets[\"emoji\"])\n",
    "\n",
    "all_emojis = np.concatenate((y_train, y_dev, y_test), axis=0)\n",
    "\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "#emojis[:10]\n",
    "\n",
    "all_emojis_idx = np.zeros(all_emojis.shape[0])\n",
    "\n",
    "for i in range (all_emojis.shape[0]):\n",
    "    all_emojis_idx[i] = emoji_to_idx[all_emojis[i]]    \n",
    "\n",
    "all_emojis_one_hot = to_categorical (all_emojis_idx)\n",
    "    \n",
    "#y_train_idx = all_emojis_one_hot[0:10000,:]\n",
    "#y_dev_idx = all_emojis_one_hot[10000:20000,:]\n",
    "#y_test_idx = all_emojis_one_hot[20000:30000,:]\n",
    "\n",
    "\n",
    "# Temp!\n",
    "y_train_idx = all_emojis_one_hot[0:100000,:]\n",
    "y_dev_idx = all_emojis_one_hot[100000:110000,:]\n",
    "y_test_idx = all_emojis_one_hot[110000:120000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_idx[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by one-hot encoding the text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = 5000\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words = num_words) # was: 1000\n",
    "tokenizer.fit_on_texts (x_train)\n",
    "\n",
    "#x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "#x_dev_sequences = tokenizer.texts_to_sequences(x_dev)\n",
    "#x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_one_hot = tokenizer.texts_to_matrix(x_train, mode='binary')\n",
    "x_dev_one_hot = tokenizer.texts_to_matrix(x_dev, mode='binary')\n",
    "x_test_one_hot = tokenizer.texts_to_matrix(x_test, mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remember to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A linear classifier\n",
    "\n",
    "As a performance baseline for more complex models, it's instructive to see how well we can do with just a linear classifier with no hidden layer. With 64 hidden units trained on 10,000 training examples, we're able to get up to about 38% accuracy on the validation set. A lower learning rate slows down the learning (although it's still very fast) but doesn't appear to do any better in terms of final accuracy before the model starts overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = y_dev_idx.shape[1] # around 64 units for 10,000 tweets\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(output_size, activation='softmax', input_shape=(num_words,))) \n",
    "model.compile(optimizer=optimizers.RMSprop(lr = 0.005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit (x_train_one_hot, y_train_idx,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_dev_one_hot, y_dev_idx),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_history(history): \n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot (epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot (epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title ('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"best validation accuracy: \", max(history.history['val_acc']))\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow neural network\n",
    "A simple shallow neural network with a layer of hidden units\n",
    "\n",
    "128 hidden units gets us up to 39% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output_classes = y_dev_idx.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(num_words,)))\n",
    "model.add(Dense(n_output_classes, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0.01,\n",
    "                              patience=10,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit (x_train_one_hot, y_train_idx,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 512,\n",
    "                    verbose=1,\n",
    "                    validation_data = (x_dev_one_hot, y_dev_idx),\n",
    "                    callbacks = [early])\n",
    "\n",
    "plot_train_history(history)\n",
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two layers of hidden units\n",
    "A slight improvement over one layer of hidden units (<1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output_classes = y_dev_idx.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, activation='relu', input_shape=(num_words,)))\n",
    "model.add(Dense(128, activation='relu', \n",
    "                kernel_regularizer = regularizers.l2(0.0025)))\n",
    "model.add(Dense(n_output_classes, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                              min_delta=0.01,\n",
    "                              patience=5,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit (x_train_one_hot, y_train_idx,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 512,\n",
    "                    verbose=1,\n",
    "                    validation_data = (x_dev_one_hot, y_dev_idx),\n",
    "                    callbacks=[early])\n",
    "\n",
    "plot_train_history(history)\n",
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layer network with dropout and regularization\n",
    "\n",
    "Takes longer to train than the network with neither. Some improvements in accuracy but it's a bit marginal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output_classes = y_dev_idx.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, activation='relu', input_shape=(num_words,)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu',\n",
    "                kernel_regularizer = regularizers.l2(0.002)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(n_output_classes, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit (x_train_one_hot, y_train_idx,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 512,\n",
    "                    verbose=1,\n",
    "                    validation_data = (x_dev_one_hot, y_dev_idx))\n",
    "\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "\n",
    "First of all, let's try just an embedding layer, into a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @mydear_bangtan: [VID] 181023 - Foi adicionada a letra “D” no outdoor misterioso do #BTS em Hollywood.\\nFormando: BTS AND... \\n\\n ILOVEPAR…'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets into lists of words\n",
    "\n",
    "maxlen = 25\n",
    "\n",
    "def convert_to_sequences(tweet_text, tokenizer, maxlen=20):\n",
    "    tweet_sequence = np.asarray(tokenizer.texts_to_sequences(tweet_text))\n",
    "    padded = pad_sequences (tweet_sequence, maxlen=maxlen)\n",
    "    return (padded)\n",
    "\n",
    "x_train_sequences = convert_to_sequences(x_train, tokenizer, maxlen=maxlen)\n",
    "x_dev_sequences = convert_to_sequences(x_dev, tokenizer, maxlen=maxlen)\n",
    "x_test_sequences = convert_to_sequences(x_test, tokenizer, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output_classes = y_dev_idx.shape[1]\n",
    "\n",
    "def simple_embedding_model(num_words, n_output_classes, n_embedding_dims = 16, max_sequence_length = 20):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # 8-dimensional embedding layer for 1,000 words\n",
    "    model.add(Embedding(num_words, n_embedding_dims, input_length = max_sequence_length, name=\"embedding\")) \n",
    "\n",
    "    # flattens 3D tensor of embeddings into 2D tensor of shape (samples, maxlen * 8)\n",
    "    model.add(Flatten()) \n",
    "\n",
    "    model.add(Dense(n_output_classes, activation = 'softmax'))\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "model = simple_embedding_model(num_words,\n",
    "                               n_output_classes, \n",
    "                               n_embedding_dims = 64, \n",
    "                               max_sequence_length = maxlen)    \n",
    "\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0.03,\n",
    "                              patience=3,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath='emoji_embedding.h5',\n",
    "                                             monitor='val_acc',\n",
    "                                             save_best_only = True)\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='tensorboard_log',\n",
    "                                          #write_grads=1,\n",
    "                                          #histogram_freq=1,\n",
    "                                          embeddings_freq=1,\n",
    "                                          embeddings_data='embedding') ## ?? How to implement this\n",
    "\n",
    "history = model.fit (x_train_sequences, y_train_idx,\n",
    "                     validation_data = (x_dev_sequences, y_dev_idx),\n",
    "                     epochs = 20,\n",
    "                     batch_size = 512,\n",
    "                     verbose=1,\n",
    "                     callbacks=[early])\n",
    "\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deeper_embedding_model(num_words,\n",
    "                           n_output_classes, \n",
    "                           n_embedding_dims = 16, \n",
    "                           max_sequence_length = 20, \n",
    "                           dense1_size = 16, \n",
    "                           dropout1_rate = 0.2,\n",
    "                           dense2_size = 16,\n",
    "                           dropout2_rate = 0.2,\n",
    "                           lambd = 0.0):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # 8-dimensional embedding layer for 1,000 words\n",
    "    model.add(Embedding(num_words, n_embedding_dims, input_length = max_sequence_length, name=\"embedding\")) \n",
    "\n",
    "    # flattens 3D tensor of embeddings into 2D tensor of shape (samples, maxlen * 8)\n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(dense1_size, activation='relu'))\n",
    "    model.add(Dropout(dropout1_rate))\n",
    "    model.add(Dense(dense2_size, activation='relu',\n",
    "                    kernel_regularizer = regularizers.l2(lambd)))\n",
    "    model.add(Dropout(dropout2_rate))\n",
    "    model.add(Dense(n_output_classes, activation = 'softmax'))\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "model = deeper_embedding_model(num_words,\n",
    "                               n_output_classes, \n",
    "                               n_embedding_dims = 28, \n",
    "                               max_sequence_length = maxlen,\n",
    "                               lambd = 0.0006)    \n",
    "\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                              min_delta=0.01,\n",
    "                              patience=5,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit (x_train_sequences, y_train_idx,\n",
    "                     validation_data = (x_dev_sequences, y_dev_idx),\n",
    "                     epochs = 50,\n",
    "                     batch_size = 512,\n",
    "                     verbose=1,\n",
    "                     callbacks=[early])\n",
    "\n",
    "plot_train_history(history)\n",
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "The embedding->shallow NN model struggles to get above 31% accuracy. Actually best results obtained when number of embedding dimensions is small: ~3-4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning parameters for a much bigger (100,000) tweet training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deeper_embedding_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f0aa4677c4d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = deeper_embedding_model(num_words,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                \u001b[0mn_output_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                \u001b[0mn_embedding_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m#196      # 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deeper_embedding_model' is not defined"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "model = deeper_embedding_model(num_words,\n",
    "                               n_output_classes, \n",
    "                               n_embedding_dims = 128,          #196      # 128\n",
    "                               max_sequence_length = maxlen,\n",
    "                               dense1_size = 128,               # 160     # 128\n",
    "                               dropout1_rate = 0.4,           # 0.35\n",
    "                               dense2_size = 96,             # 128   # 96\n",
    "                               dropout2_rate = 0.4,           # 0.35\n",
    "                               lambd = 0.0025)                # 0.0015\n",
    "\n",
    "                           \n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0.01,\n",
    "                              patience=10,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit (x_train_sequences, y_train_idx,\n",
    "                     validation_data = (x_dev_sequences, y_dev_idx),\n",
    "                     epochs = 50,\n",
    "                     batch_size = 512,\n",
    "                     verbose=1,\n",
    "                     callbacks=[early])\n",
    "\n",
    "plot_train_history(history)\n",
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_lstm_model(num_words,\n",
    "                           n_output_classes, \n",
    "                           n_embedding_dims = 16, \n",
    "                           max_sequence_length = 20, \n",
    "                           dense1_size = 16, \n",
    "                           dropout1_rate = 0.2,\n",
    "                           lambd = 0.0):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(num_words, n_embedding_dims, input_length = max_sequence_length, name=\"embedding\")) \n",
    "    model.add(LSTM(n_embedding_dims))\n",
    "    # model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(dense1_size, activation='relu'))\n",
    "    model.add(Dropout(dropout1_rate))\n",
    "    model.add(Dense(n_output_classes, activation = 'softmax'))\n",
    "    model.compile(optimizer = 'rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "100000/100000 [==============================] - 165s 2ms/step - loss: 3.5218 - acc: 0.2257 - val_loss: 3.3117 - val_acc: 0.2564\n",
      "Epoch 2/50\n",
      "100000/100000 [==============================] - 147s 1ms/step - loss: 2.9270 - acc: 0.3226 - val_loss: 2.9655 - val_acc: 0.3157\n",
      "Epoch 3/50\n",
      "100000/100000 [==============================] - 159s 2ms/step - loss: 2.6227 - acc: 0.3853 - val_loss: 2.9134 - val_acc: 0.3280\n",
      "Epoch 4/50\n",
      " 46080/100000 [============>.................] - ETA: 1:29 - loss: 2.4787 - acc: 0.4137"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "model = simple_lstm_model(num_words,\n",
    "                          n_output_classes, \n",
    "                          n_embedding_dims = 90,          \n",
    "                          max_sequence_length = maxlen,\n",
    "                          dense1_size = 128,               \n",
    "                          dropout1_rate = 0.2,           \n",
    "                          lambd = 0.0025)                \n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='tensorboard_log')\n",
    "                                          #write_grads=1,\n",
    "                                          #histogram_freq=1,\n",
    "                                          #embeddings_freq=1,\n",
    "                                          #embeddings_data='embedding') ## ?? How to implement this\n",
    "\n",
    "\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0.01,\n",
    "                              patience=5,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit (x_train_sequences, y_train_idx,\n",
    "                     validation_data = (x_dev_sequences, y_dev_idx),\n",
    "                     epochs = 50,\n",
    "                     batch_size = 512,\n",
    "                     verbose=1,\n",
    "                     callbacks=[early, tensorboard])\n",
    "\n",
    "plot_train_history(history)\n",
    "print (\"best validation accuracy: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
