{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojitranslate project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part one - learning to predict emojis from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code adapted from the Osinga deep learning cookbook - using the Twitter API to sample EN language tweets that contain exactly one emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import twitter\n",
    "import emoji\n",
    "# import itertools\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "import keras.callbacks\n",
    "#import json\n",
    "\n",
    "import os\n",
    "# import nb_utils\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Embedding, GlobalMaxPooling1D#, Merge \n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate, Average\n",
    "\n",
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😂        35746\n",
       "😭        14450\n",
       "❤        11139\n",
       "emoji     8154\n",
       "😍         7890\n",
       "🔥         7636\n",
       "🤣         4974\n",
       "🤔         3923\n",
       "🙏         3607\n",
       "😩         3584\n",
       "💕         3578\n",
       "😊         3392\n",
       "👀         3127\n",
       "💜         2936\n",
       "✨         2924\n",
       "👏         2887\n",
       "🙄         2825\n",
       "🖤         2638\n",
       "💀         2628\n",
       "😏         2554\n",
       "🎉         2374\n",
       "🙌         2310\n",
       "😘         2265\n",
       "💯         2147\n",
       "👍         2026\n",
       "💙         2013\n",
       "😉         1925\n",
       "💖         1878\n",
       "🚨         1873\n",
       "👇         1860\n",
       "         ...  \n",
       "◻            1\n",
       "🗃            1\n",
       "🀄            1\n",
       "🚛            1\n",
       "🛤            1\n",
       "⏸            1\n",
       "🈴            1\n",
       "🚸            1\n",
       "📏            1\n",
       "⛎            1\n",
       "🎍            1\n",
       "🕖            1\n",
       "🈲            1\n",
       "🏨            1\n",
       "➗            1\n",
       "◀            1\n",
       "🉐            1\n",
       "🧙            1\n",
       "🗂            1\n",
       "💹            1\n",
       "🚃            1\n",
       "🚡            1\n",
       "🖲            1\n",
       "🛋            1\n",
       "👷            1\n",
       "🏬            1\n",
       "🗜            1\n",
       "🚥            1\n",
       "🥠            1\n",
       "🧚            1\n",
       "Name: emoji, Length: 1054, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets = pd.read_csv('data/emojis_homemade.csv')\n",
    "all_tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281488, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Unnamed: 0                                               text emoji\n",
       "0              0.0  RT @mydear_bangtan: [VID] 181023 - Foi adicion...     ©\n",
       "1              1.0  RT @WuYiFan_KrisBar: 181023 Kris Wu Studio upd...     💫\n",
       "2              2.0  RT @TrendsPrabhas: Now you are watching Indian...     😎\n",
       "3              3.0                                    dats for keeps      💛\n",
       "4              4.0  RT @xruiztru: WHO ARMS THE SAUDIS?\\n\\n  🇺🇸US 5...     🏳\n",
       "5              5.0  RT @xxxfreaknasty2: interracial couple go at i...     🌝\n",
       "6              6.0               @Eric_Deshaun Holy shit no I think.      😩\n",
       "7              7.0     RT @fanmutuals: army, follow who retweet this      👑\n",
       "8              8.0  RT @_Simplykpop: Simply K-Pop harddrive dump\\n...     🤩\n",
       "9              9.0  Happy birthday nellie @thelittlegend, hope you...     ❤\n",
       "10            10.0  RT @montparnasty: \"I have passed through fire\"...     💕\n",
       "11            11.0  RT @Jason_Mckeown: Look how empty Valley Parad...     😟\n",
       "12            12.0  RT @hellohonne: Incredible to be involved in t...     😱\n",
       "13            13.0  RT @btsvotingteam: Too much feels \\n\\n#BTS #MP...     😢\n",
       "14            14.0  RT @premierleague: #PL TOP SCORERS \\n\\n7 - @ha...     🔥\n",
       "15            15.0  [DEPASCAL] RUSSEL HOODIE BLUE\\n https://t.co/C...     🔗\n",
       "16            16.0  The joys of having a make up artist as your be...     😂\n",
       "17            17.0  RT @EXOGlobal: [!] #TEMPO_KAI is currently tre...     🎊\n",
       "18            18.0     RT @fanmutuals: army, follow who retweet this      👑\n",
       "19            19.0       @slicksean Someone didn’t read the article.      🙄\n",
       "20            20.0  RT @mefeater: Happy 23rd Birthday to Duckie Th...     💕\n",
       "21            21.0  RT @followprojecten: follow everyone who retwe...     💓\n",
       "22            22.0  RT @hellohonne: Incredible to be involved in t...     😱\n",
       "23            23.0  RT @abusedmember: ⚀Re-enter the red cube⚀\\n\\n*...     🏁\n",
       "24            24.0  WOW @Just_Ceeks let me know when I can come pl...     🙃\n",
       "25            25.0  RT @MarkLand0802: markno～\\n#NCT #Mark #MarkLee...     🤟\n",
       "26            26.0  RT @SAPTechEd: Don't forget! The #SAPTechEd ne...     🍻\n",
       "27            27.0  RT @YuH8TM3:  \\nYou have all decided to push y...     🤔\n",
       "28            28.0                    Somebody stop simone biles omg      🔥\n",
       "29            29.0              @mrfairyvideo @__jim_in Wth i see...      😨\n",
       "...            ...                                                ...   ...\n",
       "281458        47.0  RT @NotEverydayPod: Bruce Jenner's new bae. Na...     😊\n",
       "281459        48.0                       @qistroll Special ke spesial     😂\n",
       "281460        49.0  @Angie18548707 I forgot how to make myself loo...     😂\n",
       "281461        50.0                                         Thank you      🔥\n",
       "281462        51.0  J.T Barrett wouldn’t of bitch out of that run ...     🙄\n",
       "281463        52.0  Despite being Leicester lad living in the Nort...     🏉\n",
       "281464        53.0  RT @gurooghantaal: ️ SexyAsFuck https://t.co/O...     ♨\n",
       "281465        54.0  RT @RampsMAGARants: @JohnJamesMI @DailyCaller ...     🙏\n",
       "281466        55.0  RT @Culinary226:  Have you voted early? Nevada...     👥\n",
       "281467        56.0               @TargetDarts @BradBrooks180 Awesome      🎯\n",
       "281468        57.0  RT @JK_Glitters: Here is another GIF I cherish...     😭\n",
       "281469        58.0  @PoznanInMyPants I don’t know why they were hi...     😂\n",
       "281470        59.0  RT @Ceedie103: Here y’all go letting junkies h...     😂\n",
       "281471        60.0      @Zarahbrees i know  i did NOT see that coming     😭\n",
       "281472        61.0  RT @CuteEmergency: nah get that out of here! \\...     📹\n",
       "281473        62.0  RT @ArianaGrande: and yet i think this’ll be y...     💭\n",
       "281474        63.0  RT @yugyeomcontent: The most beautiful  https:...     💗\n",
       "281475        64.0  RT @Ahmed_Fawzy525: You still make me smile.\\n...     ❤\n",
       "281476        65.0  RT @DailyRapFacts: Travis Scott Edition of NBA...     🔥\n",
       "281477        66.0  RT @radicalmariane: @King_James0 happy birthda...     ☺\n",
       "281478        67.0               @realstuartl @SixerSense Smart move      😂\n",
       "281479        68.0  @DonaldJTrumpJr @CNN “SUPER SELECTIVE.” CNN’s ...     🤔\n",
       "281480        69.0             @DonnaDlm71 You could of shared Donna      😔\n",
       "281481        70.0                          @lisetgarciaxo love you 🏼     🤞\n",
       "281482        71.0  @TomiLahren \"I’m not for celebrities that pop ...     😏\n",
       "281483        72.0                            Wtf?! That top 2-5 tho      😂\n",
       "281484        73.0                            That is my man, William     👌\n",
       "281485        74.0  RT @LeroySane19: Wait till the end  #LS19 #inS...     😂\n",
       "281486        75.0                She got wife written all over her.      😙\n",
       "281487        76.0  RT @koji2530112: Follow everyone who RETWEETS ...     🎀\n",
       "\n",
       "[281488 rows x 3 columns]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😂        35746\n",
       "😭        14450\n",
       "❤        11139\n",
       "emoji     8154\n",
       "😍         7890\n",
       "🔥         7636\n",
       "🤣         4974\n",
       "🤔         3923\n",
       "🙏         3607\n",
       "😩         3584\n",
       "💕         3578\n",
       "😊         3392\n",
       "👀         3127\n",
       "💜         2936\n",
       "✨         2924\n",
       "👏         2887\n",
       "🙄         2825\n",
       "🖤         2638\n",
       "💀         2628\n",
       "😏         2554\n",
       "🎉         2374\n",
       "🙌         2310\n",
       "😘         2265\n",
       "💯         2147\n",
       "👍         2026\n",
       "💙         2013\n",
       "😉         1925\n",
       "💖         1878\n",
       "🚨         1873\n",
       "👇         1860\n",
       "         ...  \n",
       "😳         1797\n",
       "😅         1670\n",
       "💪         1603\n",
       "😁         1600\n",
       "😱         1571\n",
       "‼         1505\n",
       "💛         1498\n",
       "👌         1489\n",
       "🗣         1454\n",
       "🙃         1393\n",
       "🤪         1353\n",
       "😢         1304\n",
       "☺         1287\n",
       "😤         1278\n",
       "✌         1240\n",
       "🤗         1213\n",
       "🤧         1160\n",
       "😒         1138\n",
       "💥         1131\n",
       "😈         1121\n",
       "©         1119\n",
       "💔         1114\n",
       "😫         1108\n",
       "😌         1082\n",
       "☹         1079\n",
       "💗         1069\n",
       "👉         1066\n",
       "🙂         1064\n",
       "🎶         1044\n",
       "🤩         1007\n",
       "Name: emoji, Length: 64, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\n",
    "tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @DingDongLive: LMAO  @TexasEDMFamily\\n@FreakyDeakyFam\\nGrab my hard/hybrid trap mix&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; https://t.co/34rcxAVbgk https://t.co/ArQeh23L…'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tweets['text'], key=lambda t:len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emoji', '©', '‼', '☹', '☺', '♥', '✌', '✨', '❤', '🎃']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(sorted(set(chain(*tweets['text']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in tweets['text'])\n",
    "\n",
    "emojis = list(sorted(set(tweets['emoji'])))\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "emojis[:10]\n",
    "\n",
    "#train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we've collected quite a lot of tweet data, so to speed up the prototyping of the model we're going to create a train/dev/test datasets of 10,000 tweets each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tweets = tweets[0:10000]\n",
    "dev_tweets = tweets[10000:20000]\n",
    "test_tweets = tweets[20000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]),\n",
       " array([35.,  9., 33., 17., 11., 33., 12., 37.,  8., 33.]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(tweets, batch_size):\n",
    "    while True:\n",
    "        if batch_size is None:\n",
    "            batch = tweets\n",
    "            batch_size = batch.shape[0]\n",
    "        else:\n",
    "            batch = tweets.sample(batch_size)\n",
    "        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
    "            y[row_idx] = emoji_to_idx[row['emoji']]\n",
    "            for ch_idx, ch in enumerate(row['text']):\n",
    "                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "        yield X, y\n",
    "\n",
    "next(data_generator(tweets, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_cnn_input (InputLayer)  (None, 163, 2222)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 158, 128)          1706624   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 34, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "char_cnn_predictions (Dense) (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 2,174,016\n",
      "Trainable params: 2,174,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/19 [==============================] - 328s 16s/step - loss: 3.0136 - acc: 0.3029 - val_loss: 3.1220 - val_acc: 0.3046\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If printing histograms, validation_data must be provided, and cannot be a generator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ef27e21e5ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# was: verbose=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m             raise ValueError(\"If printing histograms, validation_data must be \"\n\u001b[0m\u001b[1;32m    913\u001b[0m                              \"provided, and cannot be a generator.\")\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If printing histograms, validation_data must be provided, and cannot be a generator."
     ]
    }
   ],
   "source": [
    "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0.03,\n",
    "                              patience=2,\n",
    "                              verbose=1, mode='auto')\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath='emoji_cnn.h5',\n",
    "                                             monitor='val_acc',\n",
    "                                             save_best_only = True)\n",
    "\n",
    "#tensorboard = keras.callbacks.TensorBoard(log_dir='tensorboard_log',\n",
    "#                                         histogram_freq=1,\n",
    "#                                         embeddings_freq=1)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "char_cnn_model.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    validation_data = data_generator(dev_tweets, batch_size=BATCH_SIZE),\n",
    "    validation_steps=int(dev_tweets.shape[0]/BATCH_SIZE),\n",
    "    epochs=5,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=1, # was: verbose=2\n",
    "    callbacks=[early, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23120</th>\n",
       "      <td>RT @PGATOUR: This is as unlucky as golf gets. \\n\\n#TOURVault https://t.co/1lyERpKge8</td>\n",
       "      <td>🙄</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23904</th>\n",
       "      <td>The worst champions league atmosphere I’ve ever heard at old Trafford  #manu #overated</td>\n",
       "      <td>🙄</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24903</th>\n",
       "      <td>i think i go to school to get sick not smart</td>\n",
       "      <td>😂</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21950</th>\n",
       "      <td>Someone said the Cake Island Arc in One Piece has made them hate cake...\\nFor me, it makes me want to stuff my face with swe...</td>\n",
       "      <td>😂</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236</th>\n",
       "      <td>RT @tribranchvo: she did NOT just do the “okurrrrr” ASMR style no she did not  https://t.co/t1FtqLXUkD</td>\n",
       "      <td>💀</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16296</th>\n",
       "      <td>RT @wisconsinGodsPl: THE 1980 CUBAN BOAT LIFT WAS A IMMIGRATION CRISIS\\n\\nNOTICE THE HEADLINES IN THE PAPER\\n\\nPRESIDENT TRU...</td>\n",
       "      <td>🤔</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22613</th>\n",
       "      <td>@Humor_Silly Actually BJPWon Main Inka Haath Zyada Hai</td>\n",
       "      <td>🤣</td>\n",
       "      <td>😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17512</th>\n",
       "      <td>text</td>\n",
       "      <td>emoji</td>\n",
       "      <td>emoji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21778</th>\n",
       "      <td>RT @BillHai61993017: @RealSaavedra Imagine that, a DYING party  trying to  register  DEAD people!!!!</td>\n",
       "      <td>😅</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19871</th>\n",
       "      <td>RT @Soumz: Today is Marshmallow appreciation day. Because every day is Marshmallow appreciation day \\nأعطو حبكم للمارشملو  h...</td>\n",
       "      <td>💙</td>\n",
       "      <td>❤</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  text  \\\n",
       "23120                                             RT @PGATOUR: This is as unlucky as golf gets. \\n\\n#TOURVault https://t.co/1lyERpKge8   \n",
       "23904                                           The worst champions league atmosphere I’ve ever heard at old Trafford  #manu #overated   \n",
       "24903                                                                                    i think i go to school to get sick not smart    \n",
       "21950  Someone said the Cake Island Arc in One Piece has made them hate cake...\\nFor me, it makes me want to stuff my face with swe...   \n",
       "18236                           RT @tribranchvo: she did NOT just do the “okurrrrr” ASMR style no she did not  https://t.co/t1FtqLXUkD   \n",
       "16296  RT @wisconsinGodsPl: THE 1980 CUBAN BOAT LIFT WAS A IMMIGRATION CRISIS\\n\\nNOTICE THE HEADLINES IN THE PAPER\\n\\nPRESIDENT TRU...   \n",
       "22613                                                                          @Humor_Silly Actually BJPWon Main Inka Haath Zyada Hai    \n",
       "17512                                                                                                                             text   \n",
       "21778                             RT @BillHai61993017: @RealSaavedra Imagine that, a DYING party  trying to  register  DEAD people!!!!   \n",
       "19871  RT @Soumz: Today is Marshmallow appreciation day. Because every day is Marshmallow appreciation day \\nأعطو حبكم للمارشملو  h...   \n",
       "\n",
       "        true   pred  \n",
       "23120      🙄      ❤  \n",
       "23904      🙄      😂  \n",
       "24903      😂      😂  \n",
       "21950      😂      😂  \n",
       "18236      💀      😂  \n",
       "16296      🤔      ❤  \n",
       "22613      🤣      😂  \n",
       "17512  emoji  emoji  \n",
       "21778      😅      ❤  \n",
       "19871      💙      ❤  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 128\n",
    "inspect_tweets = dev_tweets.sample(100)\n",
    "predicted = char_cnn_model.predict_generator(data_generator(inspect_tweets, batch_size=None), steps=1)\n",
    "show = pd.DataFrame({\n",
    "    'text': inspect_tweets['text'],\n",
    "    'true': inspect_tweets['emoji'],\n",
    "    'pred': [emojis[np.argmax(x)] for x in predicted],\n",
    "})\n",
    "show = show[['text', 'true', 'pred']]\n",
    "show.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network - single Dense layer\n",
    "\n",
    "Benchmark performance with the simplest neural network we can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide up the train/dev/test sets so we're not relying on a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_tweets[\"text\"]\n",
    "y_train = np.asarray(train_tweets[\"emoji\"])\n",
    "x_dev = dev_tweets[\"text\"]\n",
    "y_dev = np.asarray(dev_tweets[\"emoji\"])\n",
    "x_test = test_tweets[\"text\"]\n",
    "y_test = np.asarray(test_tweets[\"emoji\"])\n",
    "\n",
    "all_emojis = np.concatenate((y_train, y_dev, y_test), axis=0)\n",
    "\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "#emojis[:10]\n",
    "\n",
    "all_emojis_idx = np.zeros(all_emojis.shape[0])\n",
    "\n",
    "for i in range (all_emojis.shape[0]):\n",
    "    all_emojis_idx[i] = emoji_to_idx[all_emojis[i]]    \n",
    "\n",
    "all_emojis_one_hot = to_categorical (all_emojis_idx)\n",
    "    \n",
    "y_train_idx = all_emojis_one_hot[0:10000,:]\n",
    "y_dev_idx = all_emojis_one_hot[10000:20000,:]\n",
    "y_test_idx = all_emojis_one_hot[20000:30000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_idx[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by one-hot encoding the text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "tokenizer.fit_on_texts (x_train)\n",
    "\n",
    "#x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "#x_dev_sequences = tokenizer.texts_to_sequences(x_dev)\n",
    "#x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_one_hot = tokenizer.texts_to_matrix(x_train, mode='binary')\n",
    "x_dev_one_hot = tokenizer.texts_to_matrix(x_dev, mode='binary')\n",
    "x_test_one_hot = tokenizer.texts_to_matrix(x_test, mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# remember to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 86us/step - loss: 3.9899 - acc: 0.1738 - val_loss: 3.8386 - val_acc: 0.2175\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 3.6715 - acc: 0.2107 - val_loss: 3.6105 - val_acc: 0.2208\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 3.4729 - acc: 0.2258 - val_loss: 3.5016 - val_acc: 0.2755\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 3.3450 - acc: 0.2944 - val_loss: 3.4162 - val_acc: 0.2989\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 3.2287 - acc: 0.3194 - val_loss: 3.3358 - val_acc: 0.3121\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 3.1207 - acc: 0.3363 - val_loss: 3.2617 - val_acc: 0.3283\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 3.0179 - acc: 0.3538 - val_loss: 3.1928 - val_acc: 0.3334\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 2.9209 - acc: 0.3669 - val_loss: 3.1276 - val_acc: 0.3373\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 2.8309 - acc: 0.3779 - val_loss: 3.0697 - val_acc: 0.3466\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 2.7455 - acc: 0.3907 - val_loss: 3.0158 - val_acc: 0.3513\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(1000,)))\n",
    "model.add(Dense(64, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit (x_train_one_hot, y_train_idx,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_dev_one_hot, y_dev_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-2a8a3a31eb66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit (input_train, y_train,\n",
    "                     epochs = 10,\n",
    "                     batch_size = 128,\n",
    "                     validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
